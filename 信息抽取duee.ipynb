{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用官方baseline：[https://aistudio.baidu.com/aistudio/projectdetail/1639964](http://https://aistudio.baidu.com/aistudio/projectdetail/1639964)\n",
    "\n",
    "\n",
    "信息抽取旨在从非结构化自然语言文本中提取结构化知识，如实体、关系、事件等。事件抽取的目标是对于给定的自然语言句子，根据预先指定的事件类型和论元角色，识别句子中所有目标事件类型的事件，并根据相应的论元角色集合抽取事件所对应的论元。其中目标事件类型 (event_type) 和论元角色 (role) 限定了抽取的范围，例如 (event_type：胜负，role：时间，胜者，败者，赛事名称)、(event_type：夺冠，role：夺冠事件，夺冠赛事，冠军)。\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/8df72cd00e684ee2b274696b20c64111a98e93d1dbe74ee8875e3c39cc8f4978\" width=\"600\" height=\"200\" alt=\"事件抽取\" align=center />\n",
    "</div>\n",
    "\n",
    "该示例展示了如何使用PaddleNLP快速复现[LIC2021事件抽取比赛](https://aistudio.baidu.com/aistudio/competition/detail/65)基线并进阶优化基线。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/62/10/ccc761d3e3a994703f31a4d0f93db0d13789d1c624a0cbbe9fe6439ed601/paddlenlp-2.0.5-py3-none-any.whl (435kB)\n",
      "\u001b[K     |████████████████████████████████| 440kB 15.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Collecting multiprocess (from paddlenlp)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/aa/d8/d7bbcef5c03890f5fe983d8419b0c5236af3657c5aa9bddf1991a6ed813a/multiprocess-0.70.12.2-py37-none-any.whl (112kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 33.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Collecting dill>=0.3.4 (from multiprocess->paddlenlp)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/b6/c3/973676ceb86b60835bb3978c6db67a5dc06be6cfdbd14ef0f5a13e3fc9fd/dill-0.3.4-py2.py3-none-any.whl (86kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 22.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.5)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.2.3)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->visualdl->paddlenlp) (56.2.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (7.2.0)\n",
      "\u001b[31mERROR: blackhole 1.0.1 has requirement numpy<=1.19.5, but you'll have numpy 1.20.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: dill, multiprocess, paddlenlp\n",
      "  Found existing installation: dill 0.3.3\n",
      "    Uninstalling dill-0.3.3:\n",
      "      Successfully uninstalled dill-0.3.3\n",
      "  Found existing installation: paddlenlp 2.0.0rc7\n",
      "    Uninstalling paddlenlp-2.0.0rc7:\n",
      "      Successfully uninstalled paddlenlp-2.0.0rc7\n",
      "Successfully installed dill-0.3.4 multiprocess-0.70.12.2 paddlenlp-2.0.5\n",
      "/home/aistudio/event_extraction\n"
     ]
    }
   ],
   "source": [
    "# 安装paddlenlp最新版本\n",
    "!pip install --upgrade paddlenlp\n",
    "\n",
    "%cd event_extraction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "该比赛有两个子任务，一个篇章级事件抽取任务，一个句子级事件抽取任务。\n",
    "\n",
    "## 篇章级事件抽取基线\n",
    "\n",
    "篇章级事件抽取数据集（DuEE-Fin）是金融领域篇章级别事件抽取数据集，\n",
    "共包含13个已定义好的事件类型约束和1.15万中文篇章（存在部分非目标篇章作为负样例），其中6900训练集，1150验证集和3450测试集，数据集下载[地址](https://aistudio.baidu.com/aistudio/competition/detail/65) 。\n",
    "在该数据集上基线采用基于[ERNIE](https://github.com/PaddlePaddle/ERNIE)的序列标注（sequence labeling）方案，分为基于序列标注的触发词抽取模型、基于序列标注的论元抽取模型和枚举属性分类模型，属于PipeLine模型；基于序列标注的触发词抽取模型采用BIO方式，识别触发词的位置以及对应的事件类型，基于序列标注的论元抽取模型采用BIO方式识别出事件中的论元以及对应的论元角色；枚举属性分类模型采用ernie进行分类。\n",
    "\n",
    "### 评测方法\n",
    "\n",
    "本任务采用预测论元F1值作为评价指标，对于每个篇章，采用不放回的方式给每个目标事件寻找最相似的预测事件（事件级别匹配），搜寻方式是优先寻找与目标事件的事件类型相同且角色和论元正确数量最多的预测事件\n",
    "\n",
    "f1_score = (2 * P * R) / (P + R)，其中\n",
    "\n",
    "- 预测论元正确=事件类型和角色相同且论元正确\n",
    "- P=预测论元正确数量 / 所有预测论元的数量\n",
    "- R=预测论元正确数量 / 所有人工标注论元的数量\n",
    "\n",
    "\n",
    "### 快速复现基线Step1：数据预处理并加载\n",
    "\n",
    "从比赛官网下载数据集，解压存放于data/DuEE-Fin目录下，将原始数据预处理成序列标注格式数据。\n",
    "处理之后的数据同样放在data/DuEE-Fin下，\n",
    "触发词识别数据文件存放在data/DuEE-Fin/role下，\n",
    "论元角色识别数据文件存放在data/DuEE-Fin/trigger下。\n",
    "枚举分类数据存放在data/DuEE-Fin/enum下。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check and create directory\n",
      "create dir * ./ckpt *\n",
      "create dir * ./ckpt/DuEE-Fin *\n",
      "create dir * ./submit *\n",
      "\n",
      "start DuEE-Fin data prepare\n",
      "\n",
      "=================DUEE FINANCE DATASET==============\n",
      "\n",
      "=================start schema process==============\n",
      "input path ./conf/DuEE-Fin/event_schema.json\n",
      "save trigger tag 27 at ./conf/DuEE-Fin/trigger_tag.dict\n",
      "save trigger tag 121 at ./conf/DuEE-Fin/role_tag.dict\n",
      "save enum tag 4 at ./conf/DuEE-Fin/enum_tag.dict\n",
      "=================end schema process===============\n",
      "\n",
      "=================start data process==============\n",
      "\n",
      "********** start document process **********\n",
      "train 32795 dev 5302 test 140867\n",
      "********** end document process **********\n",
      "\n",
      "********** start sentence process **********\n",
      "\n",
      "----trigger------for dir ./data/DuEE-Fin/sentence to ./data/DuEE-Fin/trigger\n",
      "train 7251 dev 1180\n",
      "\n",
      "----role------for dir ./data/DuEE-Fin/sentence to ./data/DuEE-Fin/role\n",
      "train 9441 dev 1524\n",
      "\n",
      "----enum------for dir ./data/DuEE-Fin/sentence to ./data/DuEE-Fin/enum\n",
      "train 429 dev 69\n",
      "********** end sentence process **********\n",
      "=================end data process==============\n",
      "end DuEE-Fin data prepare\n"
     ]
    }
   ],
   "source": [
    "!bash ./run_duee_fin.sh data_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "我们可以加载自定义数据集。通过继承[`paddle.io.Dataset`](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/Dataset_cn.html#dataset)，自定义实现`__getitem__` 和 `__len__`两个方法。\n",
    "\n",
    "如完成触发词识别，加载数据集event_extraction/data/DuEE-Fin/trigger。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['原', '标', '题', '：', '万', '讯', '自', '控', '(', '7', '.', '4', '9', '0', ',', '-', '0', '.', '1', '0', ',', '-', '1', '.', '3', '2', '%', ')', '：', '傅', '宇', '晨', '解', '除', '部', '分', '股', '份', '质', '押', '、', '累', '计', '质', '押', '比', '例', '为', '3', '9', '.', '5', '5', '%', '，', '，', '，', '，', '来', '源', '：', '每', '日', '经', '济', '新', '闻', '，', '每', '经', 'a', 'i', '快', '讯', '，', '万', '讯', '自', '控', '（', 's', 'z', '，', '3', '0', '0', '1', '1', '2', '，', '收', '盘', '价', '：', '7', '.', '4', '9', '元', '）', '6', '月', '3', '日', '下', '午', '发', '布', '公', '告', '称', '，', '公', '司', '接', '到', '股', '东', '傅', '宇', '晨', '的', '通', '知', '，', '获', '悉', '傅', '宇', '晨', '将', '其', '部', '分', '股', '份', '办', '理', '了', '质', '押', '业', '务', '。', '，', '截', '至', '本', '公', '告', '日', '，', '傅', '宇', '晨', '共', '持', '有', '公', '司', '股', '份', '5', '7', '9', '0', '.', '3', '8', '万', '股', '，', '占', '公', '司', '总', '股', '本', '的', '2', '0', '.', '2', '5', '%', '；', '累', '计', '质', '押', '股', '份', '2', '2', '9', '0', '万', '股', '，', '占', '傅', '宇', '晨', '持', '有', '公', '司', '股', '份', '总', '数', '的', '3', '9', '.', '5', '5', '%', '，', '占', '公', '司', '总', '股', '本', '的', '8', '.', '0', '1', '%', '。']; label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-质押', 'I-质押', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "text: ['客', '户', '端', '，', '新', '浪', '港', '股', '讯', '，', '众', '安', '集', '团', '(', '0', '.', '2', '4', '8', ',', '-', '0', '.', '0', '0', ',', '-', '0', '.', '8', '0', '%', ')', '（', '0', '0', '6', '7', '2', '.', 'h', 'k', '）', '发', '布', '公', '告', '，', '于', '2', '0', '1', '9', '年', '1', '0', '月', '1', '5', '日', '，', '公', '司', '耗', '资', '9', '4', '.', '5', '6', '万', '港', '元', '回', '购', '3', '8', '0', '.', '5', '万', '股', '，', '回', '购', '价', '格', '每', '股', '0', '.', '2', '4', '8', '-', '0', '.', '2', '4', '9', '港', '元', '。']; label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-股份回购', 'I-股份回购', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "text: ['原', '标', '题', '：', '金', '徽', '酒', '(', '6', '0', '3', '9', '1', '9', '.', 's', 'h', ')', '：', '亚', '特', '集', '团', '解', '除', '质', '押', '1', '9', '8', '0', '万', '股', '，', '，', '，', '，', '来', '源', '：', '格', '隆', '汇', '，', '格', '隆', '汇', '8', '月', '5', '日', '丨', '金', '徽', '酒', '(', '6', '0', '3', '9', '1', '9', '.', 's', 'h', ')', '公', '布', '，', '公', '司', '近', '日', '收', '到', '控', '股', '股', '东', '甘', '肃', '亚', '特', '投', '资', '集', '团', '有', '限', '公', '司', '(', '“', '亚', '特', '集', '团', '”', ')', '将', '其', '持', '有', '的', '公', '司', '部', '分', '股', '份', '解', '除', '质', '押', '的', '通', '知', '。', '，', '2', '0', '1', '8', '年', '4', '月', '9', '日', '，', '亚', '特', '集', '团', '将', '其', '持', '有', '的', '公', '司', '5', '9', '8', '0', '万', '股', '有', '限', '售', '条', '件', '股', '份', '质', '押', '给', '兰', '州', '银', '行', '股', '份', '有', '限', '公', '司', '陇', '南', '分', '行', '。']; label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-解除质押', 'I-解除质押', 'I-解除质押', 'I-解除质押', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from utils import load_dict\n",
    "\n",
    "class DuEventExtraction(paddle.io.Dataset):\n",
    "    \"\"\"DuEventExtraction\"\"\"\n",
    "    def __init__(self, data_path, tag_path):\n",
    "\n",
    "        self.label_vocab = load_dict(tag_path)\n",
    "        self.word_ids = []\n",
    "        self.label_ids = []\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            # skip the head line\n",
    "            next(fp)\n",
    "            for line in fp.readlines():\n",
    "                words, labels = line.strip('\\n').split('\\t')\n",
    "                words = words.split('\\002')\n",
    "                labels = labels.split('\\002')\n",
    "                self.word_ids.append(words)\n",
    "                self.label_ids.append(labels)\n",
    "\n",
    "        self.label_num = max(self.label_vocab.values()) + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.word_ids[index], self.label_ids[index]\n",
    "\n",
    "train_ds = DuEventExtraction('./data/DuEE-Fin/trigger/train.tsv', './conf/DuEE-Fin/trigger_tag.dict')\n",
    "dev_ds = DuEventExtraction('./data/DuEE-Fin/trigger/dev.tsv', './conf/DuEE-Fin/trigger_tag.dict')\n",
    "\n",
    "count = 0\n",
    "for text, label in train_ds:\n",
    "    print(f\"text: {text}; label: {label}\")\n",
    "    count += 1\n",
    "    if count >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 快速复现基线Step2：构建模型\n",
    "\n",
    "\n",
    "基于序列标注的触发词抽取模型是整体模型的一部分，该部分主要是给定事件类型，识别句子中出现的事件触发词对应的位置以及对应的事件类别，该模型是基于ERNIE开发序列标注模型，模型原理图如下：\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/435eb3cde281427eaefedf942dbdd425e8de5e2790884f5ebc16749fbda7b609\" width=\"500\" height=\"400\" alt=\"基于序列标注的触发词抽取模型\" align=center />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "同样地，基于序列标注的论元抽取模型也是基于ERNIE开发序列标注模型，该部分主要是识别出事件中的论元以及对应论元角色，模型原理图如下：\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/6c47ba6465784fd0a715e86c2916b943fb48e709b4104d69ab9c39cb000929a7\" width=\"500\" height=\"400\" alt=\"基于序列标注的论元抽取模型\" align=center />\n",
    "</div>\n",
    "\n",
    "上述样例中通过模型识别出：1）论元\"新东方\"，并分配标签\"B-收购方\"、\"I-收购方\"、\"I-收购方\"；2）论元\"东方优播\", 并分配标签\"B-被收购方\"、\"I-被收购方\"、\"I-被收购方\"、\"I-被收购方\"。最终识别出文本中包含的论元角色和论元对是<收购方，新东方>、<被收购方，东方优播>\n",
    "\n",
    "**PaddleNLP提供了ERNIE预训练模型常用序列标注模型，可以通过指定模型名字完成一键加载**：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-16 13:22:52,964] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-1.0\n",
      "[2021-07-16 13:22:52,967] [    INFO] - Downloading ernie_v1_chn_base.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams\n",
      " 24%|██▎       | 92742/392507 [00:02<00:07, 40303.81it/s]"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import ErnieForTokenClassification, ErnieForSequenceClassification\n",
    "\n",
    "label_map = load_dict('./conf/DuEE-Fin/trigger_tag.dict')\n",
    "id2label = {val: key for key, val in label_map.items()}\n",
    "model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "同时，对于枚举分类数据采用的是基于ERNIE的文本分类模型，枚举角色类型为环节。模型原理图如下：\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/359c24d307f9486d8b3f5646937ccef4893634f7ffa444009432693cbb2fbd25\" width=\"500\" height=\"400\" alt=\"枚举属性分类模型\" align=center />\n",
    "</div>\n",
    "\n",
    "\n",
    "给定文本，对文本进行分类，得到不同类别上的概率 筹备上市（0.8）、暂停上市（0.02）、正式上市（0.15）、终止上市（0.03）\n",
    "\n",
    "\n",
    "**同样地，PaddleNLP提供了ERNIE预训练模型常用文本分类模型，可以通过指定模型名字完成一键加载**：\n",
    "\n",
    "```python\n",
    "from paddlenlp.transformers import ErnieForSequenceClassification\n",
    "\n",
    "model = ErnieForSequenceClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_map))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 快速复现基线Step3：数据处理\n",
    "\n",
    "我们需要将原始数据处理成模型可读入的数据。PaddleNLP为了方便用户处理数据，内置了对于各个预训练模型对应的Tokenizer，可以完成\n",
    "文本token化，转token ID，文本长度截断等操作。与加载模型类似地，也可以一键加载。\n",
    "\n",
    "文本数据处理直接调用tokenizer即可输出模型所需输入数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import ErnieTokenizer, ErnieModel\n",
    "\n",
    "tokenizer = ErnieTokenizer.from_pretrained(\"ernie-1.0\")\n",
    "ernie_model = ErnieModel.from_pretrained(\"ernie-1.0\")\n",
    "\n",
    "# 一行代码完成切分token，映射token ID以及拼接特殊token\n",
    "encoded_text = tokenizer(text=\"请输入测试样例\", return_length=True, return_position_ids=True)\n",
    "for key, value in encoded_text.items():\n",
    "    print(\"{}:\\n\\t{}\".format(key, value))\n",
    "\n",
    "# 转化成paddle框架数据格式\n",
    "input_ids = paddle.to_tensor([encoded_text['input_ids']])\n",
    "print(\"input_ids : \\n\\t{}\".format(input_ids))\n",
    "\n",
    "segment_ids = paddle.to_tensor([encoded_text['token_type_ids']])\n",
    "print(\"token_type_ids : \\n\\t{}\".format(segment_ids))\n",
    "\n",
    "# 此时即可输入ERNIE模型中得到相应输出\n",
    "sequence_output, pooled_output = ernie_model(input_ids, segment_ids)\n",
    "print(\"Token wise output shape: \\n\\t{}\\nPooled output shape: \\n\\t{}\".format(sequence_output.shape, pooled_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "由以上代码可以见，tokenizer提供了一种非常便利的方式生成模型所需的数据格式。\n",
    "\n",
    "以上，\n",
    "\n",
    "* input_ids: 表示输入文本的token ID。\n",
    "* token_type_ids: 表示对应的token属于输入的第一个句子还是第二个句子。（Transformer类预训练模型支持单句以及句对输入。）详细参见左侧 sequence_labeling.py convert_example_to_feature()函数解释。\n",
    "* seq_len： 表示输入句子的token个数。\n",
    "* input_mask：表示对应的token是否一个padding token。由于一个batch中的输入句子长度不同，所以需要将不同长度的句子padding到统一固定长度。1表示真实输入，0表示对应token为padding token。\n",
    "* position_ids: 表示对应token在整个输入序列中的位置。\n",
    "\n",
    "同时，ERNIE模型输出有2个tensor。\n",
    "\n",
    "* sequence_output是对应每个输入token的语义特征表示，shape为(1, num_tokens, hidden_size)。其一般用于序列标注、问答等任务。\n",
    "* pooled_output是对应整个句子的语义特征表示，shape为(1, hidden_size)。其一般用于文本分类、信息检索等任务。\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "如需使用ernie-tiny预训练模型，则对应的tokenizer应该使用`paddlenlp.transformers.ErnieTinyTokenizer.from_pretrained('ernie-tiny')`\n",
    "\n",
    "以上代码示例展示了使用Transformer类预训练模型所需的数据处理步骤。为了更方便地使用，PaddleNLP同时提供了更加高阶API，一键即可返回模型所需数据格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "本基线将对数据作以下处理：\n",
    "\n",
    "* 将原始数据处理成模型可以读入的格式。首先使用tokenizer切词并映射词表中input ids，转化token type ids等。\n",
    "* 使用paddle.io.DataLoader接口多进程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "def convert_example_to_feature(example, tokenizer, label_vocab=None, max_seq_len=512, no_entity_label=\"O\", ignore_label=-1, is_test=False):\n",
    "    tokens, labels = example\n",
    "    tokenized_input = tokenizer(\n",
    "        tokens,\n",
    "        return_length=True,\n",
    "        is_split_into_words=True,\n",
    "        max_seq_len=max_seq_len)\n",
    "\n",
    "    input_ids = tokenized_input['input_ids']\n",
    "    token_type_ids = tokenized_input['token_type_ids']\n",
    "    seq_len = tokenized_input['seq_len']\n",
    "\n",
    "    if is_test:\n",
    "        return input_ids, token_type_ids, seq_len\n",
    "    elif label_vocab is not None:\n",
    "        labels = labels[:(max_seq_len-2)]\n",
    "        encoded_label = [no_entity_label] + labels + [no_entity_label]\n",
    "        encoded_label = [label_vocab[x] for x in encoded_label]\n",
    "        return input_ids, token_type_ids, seq_len, encoded_label\n",
    "\n",
    "\n",
    "no_entity_label = \"O\"\n",
    "# padding label value\n",
    "ignore_label = -1\n",
    "batch_size = 8\n",
    "max_seq_len = 512\n",
    "\n",
    "trans_func = partial(\n",
    "    convert_example_to_feature,\n",
    "    tokenizer=tokenizer,\n",
    "    label_vocab=train_ds.label_vocab,\n",
    "    max_seq_len=max_seq_len,\n",
    "    no_entity_label=no_entity_label,\n",
    "    ignore_label=ignore_label,\n",
    "    is_test=False)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.vocab[tokenizer.pad_token]), # input ids\n",
    "    Pad(axis=0, pad_val=tokenizer.vocab[tokenizer.pad_token]), # token type ids\n",
    "    Stack(), # sequence lens\n",
    "    Pad(axis=0, pad_val=ignore_label) # labels\n",
    "): fn(list(map(trans_func, samples)))\n",
    "\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=batchify_fn)\n",
    "dev_loader = paddle.io.DataLoader(\n",
    "    dataset=dev_ds,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**NOTE:**\n",
    "\n",
    "如果遇到显存不足的问题，可以调整`max_seq_len`和`batch_size`以适配显存大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 快速复现基线Step4：定义损失函数和优化器，开始训练\n",
    "\n",
    "在该基线上，我们选择交叉墒作为损失函数，使用[`paddle.optimizer.AdamW`](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/adamw/AdamW_cn.html#adamw)作为优化器。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, num_label, data_loader):\n",
    "    \"\"\"evaluate\"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for input_ids, seg_ids, seq_lens, labels in data_loader:\n",
    "        logits = model(input_ids, seg_ids)\n",
    "        loss = paddle.mean(criterion(logits.reshape([-1, num_label]), labels.reshape([-1])))\n",
    "        losses.append(loss.numpy())\n",
    "        preds = paddle.argmax(logits, axis=-1)\n",
    "        n_infer, n_label, n_correct = metric.compute(seq_lens, preds, labels)\n",
    "        metric.update(n_infer.numpy(), n_label.numpy(), n_correct.numpy())\n",
    "        precision, recall, f1_score = metric.accumulate()\n",
    "    avg_loss = np.mean(losses)\n",
    "    model.train()\n",
    "\n",
    "    return precision, recall, f1_score, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型参数保存路径\n",
    "!mkdir ckpt/DuEE-Fin/trigger/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "from paddlenlp.transformers import ErnieModel\n",
    "from paddlenlp.layers import LinearChainCrf, LinearChainCrfLoss\n",
    "\n",
    "\n",
    "class Model(ErnieModel):\n",
    "    def __init__(self, ernie, num_classes=2, dropout=None, gru_hidden_size=128):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # allow ernie to be config\n",
    "        self.ernie = ernie  \n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else\n",
    "                                  self.ernie.config[\"hidden_dropout_prob\"])\n",
    "        # add bi-gru\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.ernie.config[\"hidden_size\"],\n",
    "            hidden_size=gru_hidden_size,\n",
    "            direction='bidirect')\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=gru_hidden_size * 2,\n",
    "            out_features=num_classes)\n",
    "        # add crf\n",
    "        self.crf = LinearChainCrf(\n",
    "            num_classes, \n",
    "            with_start_stop_tag=False)\n",
    "        self.crf_loss = LinearChainCrfLoss(self.crf)\n",
    "        self.viterbi_decoder = ViterbiDecoder(\n",
    "            self.crf.transitions, \n",
    "            with_start_stop_tag=False)\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                attention_mask=None):\n",
    "        sequence_output, _ = self.ernie(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        bigru_output, _ = self.gru(sequence_output)\n",
    "        emission = self.fc(bigru_output)\n",
    "        _, prediction = self.viterbi_decoder(emission, lengths)\n",
    "        if labels is not None:\n",
    "            loss = self.crf_loss(emission, lengths, prediction, labels)\n",
    "            return loss, lengths, prediction, labels\n",
    "        else:\n",
    "            return lengths, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "learning_rate=5e-5\n",
    "weight_decay=0.01\n",
    "num_epoch = 20\n",
    "\n",
    "checkpoints = 'ckpt/DuEE-Fin/trigger/'\n",
    "\n",
    "num_training_steps = len(train_loader) * num_epoch\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=learning_rate,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "metric = ChunkEvaluator(label_list=train_ds.label_vocab.keys(), suffix=False)\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "\n",
    "step, best_f1 = 0, 0.0\n",
    "model.train()\n",
    "rank = paddle.distributed.get_rank()\n",
    "for epoch in range(num_epoch):\n",
    "    for idx, (input_ids, token_type_ids, seq_lens, labels) in enumerate(train_loader):\n",
    "        logits = model(input_ids, token_type_ids).reshape(\n",
    "            [-1, train_ds.label_num])\n",
    "        loss = paddle.mean(criterion(logits, labels.reshape([-1])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        loss_item = loss.numpy().item()\n",
    "        if step > 0 and step % 10 == 0 and rank == 0:\n",
    "            print(f'train epoch: {epoch} - step: {step} (total: {num_training_steps}) - loss: {loss_item:.6f}')\n",
    "        if step > 0 and step % 50 == 0 and rank == 0:\n",
    "            p, r, f1, avg_loss = evaluate(model, criterion, metric, len(label_map), dev_loader)\n",
    "            print(f'dev step: {step} - loss: {avg_loss:.5f}, precision: {p:.5f}, recall: {r:.5f}, ' \\\n",
    "                    f'f1: {f1:.5f} current best {best_f1:.5f}')\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                print(f'==============================================save best model ' \\\n",
    "                        f'best performerence {best_f1:5f}')\n",
    "                paddle.save(model.state_dict(), '{}/best.pdparams'.format(checkpoints))\n",
    "        step += 1\n",
    "\n",
    "# save the final model\n",
    "if rank == 0:\n",
    "    paddle.save(model.state_dict(), '{}/final.pdparams'.format(checkpoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "论元识别模型训练与触发词模型训练相同，只需将数据换成处理过后的论元识别数据集即可。\n",
    "可通过如下方式启动训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 触发词识别模型训练\n",
    "!bash run_duee_fin.sh trigger_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 触发词识别预测\n",
    "!bash run_duee_fin.sh trigger_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 论元识别模型训练\n",
    "!bash run_duee_fin.sh role_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 论元识别预测\n",
    "!bash run_duee_fin.sh role_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 枚举分类模型训练\n",
    "!bash run_duee_fin.sh enum_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 枚举分类预测\n",
    "!bash run_duee_fin.sh enum_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 快速复现基线Step5：数据后处理，提交结果\n",
    "\n",
    "按照比赛预测指定格式提交结果至[评测网站](https://aistudio.baidu.com/aistudio/competition/detail/65)。\n",
    "结果存放于`submit/test_duee_fin.json`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!bash run_duee_fin.sh pred_2_submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 句子级事件抽取基线\n",
    "\n",
    "\n",
    "句子级别通用领域的事件抽取数据集（[DuEE 1.0](https://aistudio.baidu.com/aistudio/competition/detail/32?isFromCcf=true)）上进行事件抽取的基线模型，该模型采用基于[ERNIE](https://github.com/PaddlePaddle/ERNIE)的序列标注（sequence labeling）方案，分为基于序列标注的触发词抽取模型和基于序列标注的论元抽取模型，属于PipeLine模型；基于序列标注的触发词抽取模型采用BIO方式，识别触发词的位置以及对应的事件类型，基于序列标注的论元抽取模型采用BIO方式识别出事件中的论元以及对应的论元角色。模型和数据处理方式与篇章级事件抽取相同，此处不再赘述。句子级别通用领域的事件抽取无枚举角色分类。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "!bash run_duee_1.sh data_prepare\n",
    "\n",
    "# 训练触发词识别模型\n",
    "!bash run_duee_1.sh trigger_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 触发词识别预测\n",
    "!bash run_duee_1.sh trigger_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 论元识别模型训练\n",
    "!bash run_duee_1.sh role_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 论元识别预测\n",
    "!bash run_duee_1.sh role_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据后处理，提交预测结果\n",
    "# 结果存放于submit/test_duee_1.json\n",
    "!bash run_duee_1.sh pred_2_submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 评测方法\n",
    "\n",
    "事件论元结果与人工标注的事件论元结果进行匹配，并按字级别匹配F1进行打分，不区分大小写，如论元有多个表述，则取多个匹配F1中的最高值\n",
    "\n",
    "f1_score = (2 * P * R) / (P + R)，其中\n",
    "\n",
    "- P=预测论元得分总和 / 所有预测论元的数量\n",
    "- R=预测论元得分总和 / 所有人工标注论元的数量\n",
    "- 预测论元得分=事件类型是否准确 * 论元角色是否准确 * 字级别匹配F1值 （*是相乘）\n",
    "- 字级别匹配F1值 = 2 * 字级别匹配P值 * 字级别匹配R值 / (字级别匹配P值 + 字级别匹配R值)\n",
    "- 字级别匹配P值 = 预测论元和人工标注论元共有字的数量/ 预测论元字数\n",
    "- 字级别匹配R值 = 预测论元和人工标注论元共有字的数量/ 人工标注论元字数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## 优化方法\n",
    "\n",
    "### 尝试更多的预训练模型\n",
    "\n",
    "基线采用的预训练模型为ERNIE，PaddleNLP提供了丰富的预训练模型，如BERT，RoBERTa，Electra，XLNet等。\n",
    "\n",
    "参考[PaddleNLP预训练模型介绍](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/transformers.md)\n",
    "\n",
    "如可以选择RoBERTa large中文模型优化模型效果，只需更换模型和tokenizer即可无缝衔接。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import RobertaForTokenClassification, RobertaTokenizer\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained(\"roberta-wwm-ext-large\", num_classes=len(label_map))\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-wwm-ext-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 修改模型网络结构\n",
    "\n",
    "对于序列标注任务，大家会想到GRU+CRF作为常用网络，如何在预训练模型基础之上增加这些网络层呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型集成\n",
    "\n",
    "使用多个模型进行训练预测，将各个模型预测结果进行融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上基线实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a0e8ca7743ea4fe9aa741682a63e767f8c48dc55981f4e44a40e0e00d3ab369e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
