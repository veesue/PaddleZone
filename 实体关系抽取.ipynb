{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业\n",
    "\n",
    "- 补全程序中的代码，理解其含义，并跑通整个项目；\n",
    "- 报名参加[千言数据集：信息抽取比赛](https://aistudio.baidu.com/aistudio/competition/detail/46)。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/09d7975351924046986e891d1b58df40795b52b05d9f406281692e168063543c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于预训练模型完成实体关系抽取\n",
    "\n",
    "信息抽取旨在从非结构化自然语言文本中提取结构化知识，如实体、关系、事件等。对于给定的自然语言句子，根据预先定义的schema集合，抽取出所有满足schema约束的SPO三元组。\n",
    "\n",
    "例如，「妻子」关系的schema定义为：      \n",
    "{      \n",
    "    S_TYPE: 人物,        \n",
    "    P: 妻子,      \n",
    "    O_TYPE: {      \n",
    "        @value: 人物       \n",
    "    }       \n",
    "}        \n",
    "\n",
    "该示例展示了如何使用PaddleNLP快速完成实体关系抽取，参与[千言信息抽取-关系抽取比赛](https://aistudio.baidu.com/aistudio/competition/detail/46)打榜。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Requirement already up-to-date: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.5->Flask-Babel>=1.0.0->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (7.2.0)\n",
      "/home/aistudio/relation_extraction\n"
     ]
    }
   ],
   "source": [
    "# 安装paddlenlp最新版本\n",
    "!pip install --upgrade paddlenlp\n",
    "\n",
    "%cd relation_extraction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 关系抽取介绍\n",
    "\n",
    "针对 DuIE2.0 任务中多条、交叠SPO这一抽取目标，比赛对标准的 'BIO' 标注进行了扩展。\n",
    "对于每个 token，根据其在实体span中的位置（包括B、I、O三种），我们为其打上三类标签，并且根据其所参与构建的predicate种类，将 B 标签进一步区分。给定 schema 集合，对于 N 种不同 predicate，以及头实体/尾实体两种情况，我们设计对应的共 2*N 种 B 标签，再合并 I 和 O 标签，故每个 token 一共有 (2*N+2) 个标签，如下图所示。\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/f984664777b241a9b43ef843c9b752f33906c8916bc146a69f7270b5858bee63\" width=\"500\" height=\"400\" alt=\"标注策略\" align=center />\n",
    "</div>\n",
    "\n",
    "### 评价方法\n",
    "\n",
    "对测试集上参评系统输出的SPO结果和人工标注的SPO结果进行精准匹配，采用F1值作为评价指标。注意，对于复杂O值类型的SPO，必须所有槽位都精确匹配才认为该SPO抽取正确。针对部分文本中存在实体别名的问题，使用百度知识图谱的别名词典来辅助评测。F1值的计算方式如下：\n",
    "\n",
    "F1 = (2 * P * R) / (P + R)，其中\n",
    "\n",
    "- P = 测试集所有句子中预测正确的SPO个数 / 测试集所有句子中预测出的SPO个数\n",
    "- R = 测试集所有句子中预测正确的SPO个数 / 测试集所有句子中人工标注的SPO个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step1：构建模型\n",
    "\n",
    "该任务可以看作一个序列标注任务，所以基线模型采用的是ERNIE序列标注模型。\n",
    "\n",
    "**PaddleNLP提供了ERNIE预训练模型常用序列标注模型，可以通过指定模型名字完成一键加载。PaddleNLP为了方便用户处理数据，内置了对于各个预训练模型对应的Tokenizer，可以完成文本token化，转token ID，文本长度截断等操作。**\n",
    "\n",
    "文本数据处理直接调用tokenizer即可输出模型所需输入数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-13 22:38:53,360] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/roberta_chn_large.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-13 22:39:01,919] [    INFO] - Found /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from paddlenlp.transformers import RobertaForTokenClassification, RobertaTokenizer\n",
    "\n",
    "label_map_path = os.path.join('data', \"predicate2id.json\")\n",
    "\n",
    "if not (os.path.exists(label_map_path) and os.path.isfile(label_map_path)):\n",
    "    sys.exit(\"{} dose not exists or is not a file.\".format(label_map_path))\n",
    "with open(label_map_path, 'r', encoding='utf8') as fp:\n",
    "    label_map = json.load(fp)\n",
    "    \n",
    "num_classes = (len(label_map.keys()) - 2) * 2 + 2\n",
    "\n",
    "# 补齐代码，理解TokenClassification接口含义，理解关系抽取标注体系和类别数由来\n",
    "model = RobertaForTokenClassification.from_pretrained(\"roberta-wwm-ext-large\",num_classes = (len(label_map.keys()) - 2) * 2 + 2)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-wwm-ext-large\")\n",
    "\n",
    "inputs = tokenizer(text=\"请输入测试样例\", max_seq_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step2：加载并处理数据\n",
    "\n",
    "\n",
    "从比赛官网下载数据集，解压存放于data/目录下并重命名为train_data.json, dev_data.json, test_data.json.\n",
    "\n",
    "我们可以加载自定义数据集。通过继承[`paddle.io.Dataset`](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/Dataset_cn.html#dataset)，自定义实现`__getitem__` 和 `__len__`两个方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List, Union, Dict\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "from tqdm import tqdm\n",
    "from paddlenlp.utils.log import logger\n",
    "\n",
    "from data_loader import parse_label, DataCollator, convert_example_to_feature\n",
    "from extract_chinese_and_punct import ChineseAndPunctuationExtractor\n",
    "\n",
    "\n",
    "class DuIEDataset(paddle.io.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset of DuIE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_ids: List[Union[List[int], np.ndarray]],\n",
    "            seq_lens: List[Union[List[int], np.ndarray]],\n",
    "            tok_to_orig_start_index: List[Union[List[int], np.ndarray]],\n",
    "            tok_to_orig_end_index: List[Union[List[int], np.ndarray]],\n",
    "            labels: List[Union[List[int], np.ndarray, List[str], List[Dict]]]):\n",
    "        super(DuIEDataset, self).__init__()\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.seq_lens = seq_lens\n",
    "        self.tok_to_orig_start_index = tok_to_orig_start_index\n",
    "        self.tok_to_orig_end_index = tok_to_orig_end_index\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        if isinstance(self.input_ids, np.ndarray):\n",
    "            return self.input_ids.shape[0]\n",
    "        else:\n",
    "            return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\n",
    "            \"input_ids\": np.array(self.input_ids[item]),\n",
    "            \"seq_lens\": np.array(self.seq_lens[item]),\n",
    "            \"tok_to_orig_start_index\":\n",
    "            np.array(self.tok_to_orig_start_index[item]),\n",
    "            \"tok_to_orig_end_index\": np.array(self.tok_to_orig_end_index[item]),\n",
    "            # If model inputs is generated in `collate_fn`, delete the data type casting.\n",
    "            \"labels\": np.array(\n",
    "                self.labels[item], dtype=np.float32),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls,\n",
    "                  file_path: Union[str, os.PathLike],\n",
    "                  tokenizer: RobertaTokenizer,\n",
    "                  max_length: Optional[int]=512,\n",
    "                  pad_to_max_length: Optional[bool]=None):\n",
    "        assert os.path.exists(file_path) and os.path.isfile(\n",
    "            file_path), f\"{file_path} dose not exists or is not a file.\"\n",
    "        label_map_path = os.path.join(\n",
    "            os.path.dirname(file_path), \"predicate2id.json\")\n",
    "        assert os.path.exists(label_map_path) and os.path.isfile(\n",
    "            label_map_path\n",
    "        ), f\"{label_map_path} dose not exists or is not a file.\"\n",
    "        with open(label_map_path, 'r', encoding='utf8') as fp:\n",
    "            label_map = json.load(fp)\n",
    "        chineseandpunctuationextractor = ChineseAndPunctuationExtractor()\n",
    "\n",
    "        input_ids, seq_lens, tok_to_orig_start_index, tok_to_orig_end_index, labels = (\n",
    "            [] for _ in range(5))\n",
    "        dataset_scale = sum(1 for line in open(file_path, 'r'))\n",
    "        logger.info(\"Preprocessing data, loaded from %s\" % file_path)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "            lines = fp.readlines()\n",
    "            for line in tqdm(lines):\n",
    "                example = json.loads(line)\n",
    "                input_feature = convert_example_to_feature(\n",
    "                    example, tokenizer, chineseandpunctuationextractor,\n",
    "                    label_map, max_length, pad_to_max_length)\n",
    "                input_ids.append(input_feature.input_ids)\n",
    "                seq_lens.append(input_feature.seq_len)\n",
    "                tok_to_orig_start_index.append(\n",
    "                    input_feature.tok_to_orig_start_index)\n",
    "                tok_to_orig_end_index.append(\n",
    "                    input_feature.tok_to_orig_end_index)\n",
    "                labels.append(input_feature.labels)\n",
    "\n",
    "        return cls(input_ids, seq_lens, tok_to_orig_start_index,\n",
    "                   tok_to_orig_end_index, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-13 22:39:01,989] [    INFO] - Preprocessing data, loaded from data/train_data.json\n",
      "100%|██████████| 10010/10010 [00:19<00:00, 501.59it/s]\n",
      "[2021-06-13 22:39:21,976] [    INFO] - Preprocessing data, loaded from data/dev_data.json\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 512.75it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data'\n",
    "batch_size = 32\n",
    "max_seq_length = 128\n",
    "\n",
    "train_file_path = os.path.join(data_path, 'train_data.json')\n",
    "train_dataset = DuIEDataset.from_file(\n",
    "    train_file_path, tokenizer, max_seq_length, True)\n",
    "train_batch_sampler = paddle.io.BatchSampler(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "collator = DataCollator()\n",
    "train_data_loader = paddle.io.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    collate_fn=collator)\n",
    "\n",
    "eval_file_path = os.path.join(data_path, 'dev_data.json')\n",
    "test_dataset = DuIEDataset.from_file(\n",
    "    eval_file_path, tokenizer, max_seq_length, True)\n",
    "test_batch_sampler = paddle.io.BatchSampler(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step3：定义损失函数和优化器，开始训练\n",
    "\n",
    "我们选择均方误差作为损失函数，使用[`paddle.optimizer.AdamW`](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/adamw/AdamW_cn.html#adamw)作为优化器。\n",
    "\n",
    "\n",
    "\n",
    "在训练过程中，模型保存在当前目录checkpoints文件夹下。同时在训练的同时使用官方评测脚本进行评估，输出P/R/F1指标。\n",
    "在验证集上F1可以达到69.42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "\n",
    "class BCELossForDuIE(nn.Layer):\n",
    "    def __init__(self, ):\n",
    "        super(BCELossForDuIE, self).__init__()\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, labels, mask):\n",
    "        loss = self.criterion(logits, labels)\n",
    "        mask = paddle.cast(mask, 'float32')\n",
    "        loss = loss * mask.unsqueeze(-1)\n",
    "        loss = paddle.sum(loss.mean(axis=2), axis=1) / paddle.sum(mask, axis=1)\n",
    "        loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import write_prediction_results, get_precision_recall_f1, decoding\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, data_loader, file_path, mode):\n",
    "    \"\"\"\n",
    "    mode eval:\n",
    "    eval on development set and compute P/R/F1, called between training.\n",
    "    mode predict:\n",
    "    eval on development / test set, then write predictions to \\\n",
    "        predict_test.json and predict_test.json.zip \\\n",
    "        under /home/aistudio/relation_extraction/data dir for later submission or evaluation.\n",
    "    \"\"\"\n",
    "    example_all = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            example_all.append(json.loads(line))\n",
    "    id2spo_path = os.path.join(os.path.dirname(file_path), \"id2spo.json\")\n",
    "    with open(id2spo_path, 'r', encoding='utf8') as fp:\n",
    "        id2spo = json.load(fp)\n",
    "\n",
    "    model.eval()\n",
    "    loss_all = 0\n",
    "    eval_steps = 0\n",
    "    formatted_outputs = []\n",
    "    current_idx = 0\n",
    "    for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "        eval_steps += 1\n",
    "        input_ids, seq_len, tok_to_orig_start_index, tok_to_orig_end_index, labels = batch\n",
    "        logits = model(input_ids=input_ids)\n",
    "        mask = (input_ids != 0).logical_and((input_ids != 1)).logical_and((input_ids != 2))\n",
    "        loss = criterion(logits, labels, mask)\n",
    "        loss_all += loss.numpy().item()\n",
    "        probs = F.sigmoid(logits)\n",
    "        logits_batch = probs.numpy()\n",
    "        seq_len_batch = seq_len.numpy()\n",
    "        tok_to_orig_start_index_batch = tok_to_orig_start_index.numpy()\n",
    "        tok_to_orig_end_index_batch = tok_to_orig_end_index.numpy()\n",
    "        formatted_outputs.extend(decoding(example_all[current_idx: current_idx+len(logits)],\n",
    "                                          id2spo,\n",
    "                                          logits_batch,\n",
    "                                          seq_len_batch,\n",
    "                                          tok_to_orig_start_index_batch,\n",
    "                                          tok_to_orig_end_index_batch))\n",
    "        current_idx = current_idx+len(logits)\n",
    "    loss_avg = loss_all / eval_steps\n",
    "    print(\"eval loss: %f\" % (loss_avg))\n",
    "\n",
    "    if mode == \"predict\":\n",
    "        predict_file_path = os.path.join(\"/home/aistudio/relation_extraction/data\", 'predictions.json')\n",
    "    else:\n",
    "        predict_file_path = os.path.join(\"/home/aistudio/relation_extraction/data\", 'predict_eval.json')\n",
    "\n",
    "    predict_zipfile_path = write_prediction_results(formatted_outputs,\n",
    "                                                    predict_file_path)\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        precision, recall, f1 = get_precision_recall_f1(file_path,\n",
    "                                                        predict_zipfile_path)\n",
    "        os.system('rm {} {}'.format(predict_file_path, predict_zipfile_path))\n",
    "        return precision, recall, f1\n",
    "    elif mode != \"predict\":\n",
    "        raise Exception(\"wrong mode for eval func\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 10\n",
    "warmup_ratio = 0.06\n",
    "\n",
    "criterion = BCELossForDuIE()\n",
    "# Defines learning rate strategy.\n",
    "steps_by_epoch = len(train_data_loader)\n",
    "num_training_steps = steps_by_epoch * num_train_epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_ratio)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ export BATCH_SIZE=8\n",
      "+ BATCH_SIZE=8\n",
      "+ export LR=2e-5\n",
      "+ LR=2e-5\n",
      "+ export EPOCH=12\n",
      "+ EPOCH=12\n",
      "+ unset CUDA_VISIBLE_DEVICES\n",
      "+ python -m paddle.distributed.launch --gpus 0 run_duie.py --device gpu --seed 42 --do_train --data_path ./data --max_seq_length 128 --batch_size 8 --num_train_epochs 12 --learning_rate 2e-5 --warmup_ratio 0.06 --output_dir ./checkpoints\n",
      "-----------  Configuration Arguments -----------\n",
      "gpus: 0\n",
      "heter_worker_num: None\n",
      "heter_workers: \n",
      "http_port: None\n",
      "ips: 127.0.0.1\n",
      "log_dir: log\n",
      "nproc_per_node: None\n",
      "run_mode: None\n",
      "server_num: None\n",
      "servers: \n",
      "training_script: run_duie.py\n",
      "training_script_args: ['--device', 'gpu', '--seed', '42', '--do_train', '--data_path', './data', '--max_seq_length', '128', '--batch_size', '8', '--num_train_epochs', '12', '--learning_rate', '2e-5', '--warmup_ratio', '0.06', '--output_dir', './checkpoints']\n",
      "worker_num: None\n",
      "workers: \n",
      "------------------------------------------------\n",
      "WARNING 2021-06-13 22:39:25,544 launch.py:357] Not found distinct arguments and compiled with cuda or xpu. Default use collective mode\n",
      "launch train in GPU mode!\n",
      "INFO 2021-06-13 22:39:25,546 launch_utils.py:510] Local start 1 processes. First process distributed environment info (Only For Debug): \n",
      "    +=======================================================================================+\n",
      "    |                        Distributed Envs                      Value                    |\n",
      "    +---------------------------------------------------------------------------------------+\n",
      "    |                       PADDLE_TRAINER_ID                        0                      |\n",
      "    |                 PADDLE_CURRENT_ENDPOINT                 127.0.0.1:47835               |\n",
      "    |                     PADDLE_TRAINERS_NUM                        1                      |\n",
      "    |                PADDLE_TRAINER_ENDPOINTS                 127.0.0.1:47835               |\n",
      "    |                     PADDLE_RANK_IN_NODE                        0                      |\n",
      "    |                 PADDLE_LOCAL_DEVICE_IDS                        0                      |\n",
      "    |                 PADDLE_WORLD_DEVICE_IDS                        0                      |\n",
      "    |                     FLAGS_selected_gpus                        0                      |\n",
      "    |             FLAGS_selected_accelerators                        0                      |\n",
      "    +=======================================================================================+\n",
      "\n",
      "INFO 2021-06-13 22:39:25,546 launch_utils.py:514] details abouts PADDLE_TRAINER_ENDPOINTS can be found in log/endpoints.log, and detail running logs maybe found in log/workerlog.0\n",
      "launch proc_id:5700 idx:0\n",
      "[2021-06-13 22:39:26,950] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/roberta_chn_large.pdparams\n",
      "W0613 22:39:26.951687  5700 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0613 22:39:26.956423  5700 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/parallel.py:515: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.\n",
      "  warnings.warn(\"The program will return to single-card operation. \"\n",
      "[2021-06-13 22:39:35,373] [    INFO] - Found /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/vocab.txt\n",
      "[2021-06-13 22:39:35,410] [    INFO] - Preprocessing data, loaded from ./data/train_data.json\n",
      "\n",
      "  0%|          | 0/10010 [00:00<?, ?it/s]\n",
      "  1%|          | 57/10010 [00:00<00:17, 559.25it/s]\n",
      "  1%|          | 115/10010 [00:00<00:17, 561.96it/s]\n",
      "  2%|▏         | 171/10010 [00:00<00:17, 559.47it/s]\n",
      "  2%|▏         | 231/10010 [00:00<00:17, 569.83it/s]\n",
      "  3%|▎         | 290/10010 [00:00<00:16, 572.84it/s]\n",
      "  3%|▎         | 349/10010 [00:00<00:16, 577.21it/s]\n",
      "  4%|▍         | 407/10010 [00:00<00:16, 577.58it/s]\n",
      "  5%|▍         | 461/10010 [00:00<00:16, 563.27it/s]\n",
      "  5%|▌         | 518/10010 [00:00<00:16, 563.50it/s]\n",
      "  6%|▌         | 575/10010 [00:01<00:16, 564.07it/s]\n",
      "  6%|▋         | 632/10010 [00:01<00:16, 565.79it/s]\n",
      "  7%|▋         | 688/10010 [00:01<00:16, 556.62it/s]\n",
      "  7%|▋         | 747/10010 [00:01<00:16, 566.13it/s]\n",
      "  8%|▊         | 804/10010 [00:01<00:16, 560.47it/s]\n",
      "  9%|▊         | 861/10010 [00:01<00:16, 559.49it/s]\n",
      "  9%|▉         | 920/10010 [00:01<00:16, 565.79it/s]\n",
      " 10%|▉         | 977/10010 [00:01<00:15, 565.16it/s]\n",
      " 10%|█         | 1034/10010 [00:01<00:15, 563.73it/s]\n",
      " 11%|█         | 1092/10010 [00:01<00:15, 567.77it/s]\n",
      " 11%|█▏        | 1149/10010 [00:02<00:16, 550.07it/s]\n",
      " 12%|█▏        | 1206/10010 [00:02<00:15, 553.02it/s]\n",
      " 13%|█▎        | 1262/10010 [00:02<00:15, 552.52it/s]\n",
      " 13%|█▎        | 1319/10010 [00:02<00:15, 554.34it/s]\n",
      " 14%|█▎        | 1376/10010 [00:02<00:15, 558.82it/s]\n",
      " 14%|█▍        | 1433/10010 [00:02<00:15, 560.77it/s]\n",
      " 15%|█▍        | 1490/10010 [00:02<00:15, 562.20it/s]\n",
      " 15%|█▌        | 1549/10010 [00:02<00:14, 565.10it/s]\n",
      " 16%|█▌        | 1606/10010 [00:02<00:14, 563.93it/s]\n",
      " 17%|█▋        | 1663/10010 [00:02<00:14, 562.26it/s]\n",
      " 17%|█▋        | 1720/10010 [00:03<00:14, 555.03it/s]\n",
      " 18%|█▊        | 1777/10010 [00:03<00:14, 558.94it/s]\n",
      " 18%|█▊        | 1834/10010 [00:03<00:14, 558.97it/s]\n",
      " 19%|█▉        | 1891/10010 [00:03<00:14, 561.81it/s]\n",
      " 19%|█▉        | 1948/10010 [00:03<00:14, 563.82it/s]\n",
      " 20%|██        | 2007/10010 [00:03<00:14, 568.56it/s]\n",
      " 21%|██        | 2064/10010 [00:03<00:14, 563.03it/s]\n",
      " 21%|██        | 2121/10010 [00:03<00:14, 561.49it/s]\n",
      " 22%|██▏       | 2179/10010 [00:03<00:13, 565.69it/s]\n",
      " 22%|██▏       | 2236/10010 [00:03<00:13, 562.40it/s]\n",
      " 23%|██▎       | 2293/10010 [00:04<00:13, 558.71it/s]\n",
      " 23%|██▎       | 2349/10010 [00:04<00:13, 553.67it/s]\n",
      " 24%|██▍       | 2406/10010 [00:04<00:13, 557.31it/s]\n",
      " 25%|██▍       | 2464/10010 [00:04<00:13, 562.27it/s]\n",
      " 25%|██▌       | 2521/10010 [00:04<00:13, 559.36it/s]\n",
      " 26%|██▌       | 2578/10010 [00:04<00:13, 559.88it/s]\n",
      " 26%|██▋       | 2635/10010 [00:04<00:13, 558.53it/s]\n",
      " 27%|██▋       | 2691/10010 [00:04<00:13, 556.33it/s]\n",
      " 27%|██▋       | 2747/10010 [00:04<00:13, 552.87it/s]\n",
      " 28%|██▊       | 2803/10010 [00:04<00:13, 552.33it/s]\n",
      " 29%|██▊       | 2859/10010 [00:05<00:12, 554.30it/s]\n",
      " 29%|██▉       | 2916/10010 [00:05<00:12, 557.80it/s]\n",
      " 30%|██▉       | 2972/10010 [00:05<00:12, 554.65it/s]\n",
      " 30%|███       | 3028/10010 [00:05<00:12, 546.33it/s]\n",
      " 31%|███       | 3083/10010 [00:05<00:12, 545.74it/s]\n",
      " 31%|███▏      | 3141/10010 [00:05<00:12, 554.76it/s]\n",
      " 32%|███▏      | 3199/10010 [00:05<00:12, 561.84it/s]\n",
      " 33%|███▎      | 3256/10010 [00:05<00:12, 562.07it/s]\n",
      " 33%|███▎      | 3313/10010 [00:05<00:11, 562.75it/s]\n",
      " 34%|███▎      | 3370/10010 [00:06<00:11, 555.32it/s]\n",
      " 34%|███▍      | 3426/10010 [00:06<00:11, 551.01it/s]\n",
      " 35%|███▍      | 3483/10010 [00:06<00:11, 555.39it/s]\n",
      " 35%|███▌      | 3540/10010 [00:06<00:11, 559.11it/s]\n",
      " 36%|███▌      | 3598/10010 [00:06<00:11, 562.60it/s]\n",
      " 37%|███▋      | 3655/10010 [00:06<00:11, 555.43it/s]\n",
      " 37%|███▋      | 3711/10010 [00:06<00:11, 556.76it/s]\n",
      " 38%|███▊      | 3767/10010 [00:06<00:11, 548.72it/s]\n",
      " 38%|███▊      | 3822/10010 [00:06<00:11, 540.18it/s]\n",
      " 39%|███▊      | 3877/10010 [00:06<00:11, 540.15it/s]\n",
      " 39%|███▉      | 3934/10010 [00:07<00:11, 547.31it/s]\n",
      " 40%|███▉      | 3989/10010 [00:07<00:11, 544.10it/s]\n",
      " 40%|████      | 4044/10010 [00:07<00:10, 544.99it/s]\n",
      " 41%|████      | 4103/10010 [00:07<00:10, 554.58it/s]\n",
      " 42%|████▏     | 4160/10010 [00:07<00:10, 558.85it/s]\n",
      " 42%|████▏     | 4216/10010 [00:07<00:10, 554.88it/s]\n",
      " 43%|████▎     | 4272/10010 [00:07<00:10, 548.53it/s]\n",
      " 43%|████▎     | 4328/10010 [00:07<00:10, 551.07it/s]\n",
      " 44%|████▍     | 4386/10010 [00:07<00:10, 557.58it/s]\n",
      " 44%|████▍     | 4443/10010 [00:07<00:09, 561.16it/s]\n",
      " 45%|████▍     | 4502/10010 [00:08<00:09, 567.09it/s]\n",
      " 46%|████▌     | 4559/10010 [00:08<00:09, 567.15it/s]\n",
      " 46%|████▌     | 4616/10010 [00:08<00:09, 565.83it/s]\n",
      " 47%|████▋     | 4673/10010 [00:08<00:09, 562.56it/s]\n",
      " 47%|████▋     | 4730/10010 [00:08<00:09, 559.57it/s]\n",
      " 48%|████▊     | 4786/10010 [00:08<00:09, 552.76it/s]\n",
      " 48%|████▊     | 4845/10010 [00:08<00:09, 560.37it/s]\n",
      " 49%|████▉     | 4902/10010 [00:08<00:09, 555.99it/s]\n",
      " 50%|████▉     | 4963/10010 [00:08<00:08, 569.07it/s]\n",
      " 50%|█████     | 5021/10010 [00:08<00:08, 561.42it/s]\n",
      " 51%|█████     | 5078/10010 [00:09<00:08, 553.61it/s]\n",
      " 51%|█████▏    | 5134/10010 [00:09<00:08, 549.37it/s]\n",
      " 52%|█████▏    | 5191/10010 [00:09<00:08, 553.65it/s]\n",
      " 52%|█████▏    | 5247/10010 [00:09<00:08, 548.88it/s]\n",
      " 53%|█████▎    | 5302/10010 [00:09<00:08, 537.36it/s]\n",
      " 54%|█████▎    | 5357/10010 [00:09<00:08, 539.02it/s]\n",
      " 54%|█████▍    | 5415/10010 [00:09<00:08, 550.20it/s]\n",
      " 55%|█████▍    | 5472/10010 [00:09<00:08, 552.44it/s]\n",
      " 55%|█████▌    | 5528/10010 [00:09<00:08, 550.24it/s]\n",
      " 56%|█████▌    | 5584/10010 [00:10<00:08, 550.31it/s]\n",
      " 56%|█████▋    | 5640/10010 [00:10<00:07, 552.77it/s]\n",
      " 57%|█████▋    | 5696/10010 [00:10<00:07, 552.08it/s]\n",
      " 57%|█████▋    | 5755/10010 [00:10<00:07, 560.11it/s]\n",
      " 58%|█████▊    | 5812/10010 [00:10<00:07, 551.35it/s]\n",
      " 59%|█████▊    | 5868/10010 [00:10<00:07, 552.68it/s]\n",
      " 59%|█████▉    | 5924/10010 [00:10<00:07, 551.27it/s]\n",
      " 60%|█████▉    | 5980/10010 [00:10<00:07, 549.01it/s]\n",
      " 60%|██████    | 6036/10010 [00:10<00:07, 550.52it/s]\n",
      " 61%|██████    | 6092/10010 [00:10<00:07, 549.52it/s]\n",
      " 61%|██████▏   | 6152/10010 [00:11<00:06, 562.55it/s]\n",
      " 62%|██████▏   | 6210/10010 [00:11<00:06, 565.32it/s]\n",
      " 63%|██████▎   | 6267/10010 [00:11<00:06, 562.39it/s]\n",
      " 63%|██████▎   | 6325/10010 [00:11<00:06, 565.73it/s]\n",
      " 64%|██████▍   | 6382/10010 [00:11<00:06, 564.51it/s]\n",
      " 64%|██████▍   | 6439/10010 [00:11<00:06, 566.04it/s]\n",
      " 65%|██████▍   | 6499/10010 [00:11<00:06, 574.72it/s]\n",
      " 66%|██████▌   | 6557/10010 [00:11<00:06, 558.30it/s]\n",
      " 66%|██████▌   | 6613/10010 [00:11<00:06, 558.48it/s]\n",
      " 67%|██████▋   | 6669/10010 [00:11<00:06, 555.98it/s]\n",
      " 67%|██████▋   | 6727/10010 [00:12<00:05, 561.86it/s]\n",
      " 68%|██████▊   | 6784/10010 [00:12<00:05, 559.33it/s]\n",
      " 68%|██████▊   | 6840/10010 [00:12<00:05, 553.58it/s]\n",
      " 69%|██████▉   | 6896/10010 [00:12<00:05, 554.60it/s]\n",
      " 69%|██████▉   | 6954/10010 [00:12<00:05, 561.15it/s]\n",
      " 70%|███████   | 7011/10010 [00:12<00:05, 556.76it/s]\n",
      " 71%|███████   | 7067/10010 [00:12<00:05, 555.62it/s]\n",
      " 71%|███████   | 7125/10010 [00:12<00:05, 560.82it/s]\n",
      " 72%|███████▏  | 7182/10010 [00:12<00:05, 560.41it/s]\n",
      " 72%|███████▏  | 7239/10010 [00:12<00:04, 562.46it/s]\n",
      " 73%|███████▎  | 7298/10010 [00:13<00:04, 569.99it/s]\n",
      " 73%|███████▎  | 7356/10010 [00:13<00:04, 563.08it/s]\n",
      " 74%|███████▍  | 7413/10010 [00:13<00:04, 561.60it/s]\n",
      " 75%|███████▍  | 7470/10010 [00:13<00:04, 559.58it/s]\n",
      " 75%|███████▌  | 7529/10010 [00:13<00:04, 566.05it/s]\n",
      " 76%|███████▌  | 7587/10010 [00:13<00:04, 569.37it/s]\n",
      " 76%|███████▋  | 7644/10010 [00:13<00:04, 567.23it/s]\n",
      " 77%|███████▋  | 7702/10010 [00:13<00:04, 568.34it/s]\n",
      " 78%|███████▊  | 7761/10010 [00:13<00:03, 572.79it/s]\n",
      " 78%|███████▊  | 7819/10010 [00:13<00:03, 571.27it/s]\n",
      " 79%|███████▊  | 7877/10010 [00:14<00:03, 568.82it/s]\n",
      " 79%|███████▉  | 7935/10010 [00:14<00:03, 568.94it/s]\n",
      " 80%|███████▉  | 7995/10010 [00:14<00:03, 577.30it/s]\n",
      " 80%|████████  | 8053/10010 [00:14<00:03, 573.23it/s]\n",
      " 81%|████████  | 8112/10010 [00:14<00:03, 577.20it/s]\n",
      " 82%|████████▏ | 8170/10010 [00:14<00:03, 572.95it/s]\n",
      " 82%|████████▏ | 8228/10010 [00:14<00:03, 564.00it/s]\n",
      " 83%|████████▎ | 8285/10010 [00:14<00:03, 556.24it/s]\n",
      " 83%|████████▎ | 8342/10010 [00:14<00:02, 559.37it/s]\n",
      " 84%|████████▍ | 8402/10010 [00:15<00:02, 569.31it/s]\n",
      " 85%|████████▍ | 8464/10010 [00:15<00:02, 583.10it/s]\n",
      " 85%|████████▌ | 8523/10010 [00:15<00:02, 581.17it/s]\n",
      " 86%|████████▌ | 8582/10010 [00:15<00:02, 583.35it/s]\n",
      " 86%|████████▋ | 8641/10010 [00:15<00:02, 580.88it/s]\n",
      " 87%|████████▋ | 8700/10010 [00:15<00:02, 574.71it/s]\n",
      " 88%|████████▊ | 8759/10010 [00:15<00:02, 577.11it/s]\n",
      " 88%|████████▊ | 8817/10010 [00:15<00:02, 558.87it/s]\n",
      " 89%|████████▊ | 8874/10010 [00:15<00:02, 561.58it/s]\n",
      " 89%|████████▉ | 8933/10010 [00:15<00:01, 565.26it/s]\n",
      " 90%|████████▉ | 8991/10010 [00:16<00:01, 567.12it/s]\n",
      " 90%|█████████ | 9049/10010 [00:16<00:01, 568.83it/s]\n",
      " 91%|█████████ | 9108/10010 [00:16<00:01, 572.85it/s]\n",
      " 92%|█████████▏| 9166/10010 [00:16<00:01, 573.87it/s]\n",
      " 92%|█████████▏| 9224/10010 [00:16<00:01, 563.22it/s]\n",
      " 93%|█████████▎| 9281/10010 [00:16<00:01, 561.41it/s]\n",
      " 93%|█████████▎| 9338/10010 [00:16<00:01, 555.20it/s]\n",
      " 94%|█████████▍| 9394/10010 [00:16<00:01, 553.79it/s]\n",
      " 94%|█████████▍| 9450/10010 [00:16<00:01, 552.89it/s]\n",
      " 95%|█████████▍| 9508/10010 [00:16<00:00, 559.97it/s]\n",
      " 96%|█████████▌| 9565/10010 [00:17<00:00, 556.89it/s]\n",
      " 96%|█████████▌| 9621/10010 [00:17<00:00, 552.95it/s]\n",
      " 97%|█████████▋| 9680/10010 [00:17<00:00, 563.30it/s]\n",
      " 97%|█████████▋| 9737/10010 [00:17<00:00, 554.24it/s]\n",
      " 98%|█████████▊| 9793/10010 [00:17<00:00, 549.01it/s]\n",
      " 98%|█████████▊| 9848/10010 [00:17<00:00, 544.38it/s]\n",
      " 99%|█████████▉| 9906/10010 [00:17<00:00, 552.36it/s]\n",
      "100%|█████████▉| 9963/10010 [00:17<00:00, 555.00it/s]\n",
      "100%|██████████| 10010/10010 [00:17<00:00, 560.21it/s]\n",
      "[2021-06-13 22:39:53,305] [    INFO] - Preprocessing data, loaded from ./data/dev_data.json\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "  6%|▌         | 55/1000 [00:00<00:01, 546.40it/s]\n",
      " 11%|█         | 111/1000 [00:00<00:01, 549.82it/s]\n",
      " 17%|█▋        | 168/1000 [00:00<00:01, 553.80it/s]\n",
      " 23%|██▎       | 226/1000 [00:00<00:01, 561.08it/s]\n",
      " 29%|██▊       | 286/1000 [00:00<00:01, 571.04it/s]\n",
      " 35%|███▍      | 346/1000 [00:00<00:01, 577.70it/s]\n",
      " 40%|████      | 405/1000 [00:00<00:01, 580.63it/s]\n",
      " 46%|████▌     | 461/1000 [00:00<00:00, 572.27it/s]\n",
      " 52%|█████▏    | 517/1000 [00:00<00:00, 566.38it/s]\n",
      " 58%|█████▊    | 578/1000 [00:01<00:00, 575.92it/s]\n",
      " 64%|██████▍   | 638/1000 [00:01<00:00, 579.63it/s]\n",
      " 70%|██████▉   | 698/1000 [00:01<00:00, 583.87it/s]\n",
      " 76%|███████▌  | 756/1000 [00:01<00:00, 579.34it/s]\n",
      " 82%|████████▏ | 816/1000 [00:01<00:00, 584.20it/s]\n",
      " 88%|████████▊ | 875/1000 [00:01<00:00, 575.71it/s]\n",
      " 94%|█████████▎| 935/1000 [00:01<00:00, 581.96it/s]\n",
      " 99%|█████████▉| 994/1000 [00:01<00:00, 568.83it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 574.79it/s]\n",
      "\n",
      "=====start training of 0 epochs=====\n",
      "epoch: 0 / 12, steps: 49 / 1251, loss: 0.690860, speed: 4.68 step/s\n",
      "epoch: 0 / 12, steps: 99 / 1251, loss: 0.499369, speed: 4.89 step/s\n",
      "epoch: 0 / 12, steps: 149 / 1251, loss: 0.200860, speed: 4.81 step/s\n",
      "epoch: 0 / 12, steps: 199 / 1251, loss: 0.151082, speed: 4.86 step/s\n",
      "epoch: 0 / 12, steps: 249 / 1251, loss: 0.126003, speed: 4.88 step/s\n",
      "epoch: 0 / 12, steps: 299 / 1251, loss: 0.106503, speed: 4.88 step/s\n",
      "epoch: 0 / 12, steps: 349 / 1251, loss: 0.090383, speed: 4.87 step/s\n",
      "epoch: 0 / 12, steps: 399 / 1251, loss: 0.073157, speed: 4.78 step/s\n",
      "epoch: 0 / 12, steps: 449 / 1251, loss: 0.064143, speed: 4.86 step/s\n",
      "epoch: 0 / 12, steps: 499 / 1251, loss: 0.053789, speed: 4.86 step/s\n",
      "epoch: 0 / 12, steps: 549 / 1251, loss: 0.046160, speed: 4.84 step/s\n",
      "epoch: 0 / 12, steps: 599 / 1251, loss: 0.037923, speed: 4.87 step/s\n",
      "epoch: 0 / 12, steps: 649 / 1251, loss: 0.035627, speed: 4.75 step/s\n",
      "epoch: 0 / 12, steps: 699 / 1251, loss: 0.028445, speed: 4.83 step/s\n",
      "epoch: 0 / 12, steps: 749 / 1251, loss: 0.023687, speed: 4.87 step/s\n",
      "epoch: 0 / 12, steps: 799 / 1251, loss: 0.022658, speed: 4.84 step/s\n",
      "epoch: 0 / 12, steps: 849 / 1251, loss: 0.018600, speed: 4.85 step/s\n",
      "epoch: 0 / 12, steps: 899 / 1251, loss: 0.015312, speed: 4.76 step/s\n",
      "epoch: 0 / 12, steps: 949 / 1251, loss: 0.015191, speed: 4.85 step/s\n",
      "epoch: 0 / 12, steps: 999 / 1251, loss: 0.013224, speed: 4.86 step/s\n",
      "epoch: 0 / 12, steps: 1049 / 1251, loss: 0.012382, speed: 4.87 step/s\n",
      "epoch: 0 / 12, steps: 1099 / 1251, loss: 0.012515, speed: 4.85 step/s\n",
      "epoch: 0 / 12, steps: 1149 / 1251, loss: 0.009779, speed: 4.85 step/s\n",
      "epoch: 0 / 12, steps: 1199 / 1251, loss: 0.011654, speed: 4.76 step/s\n",
      "epoch: 0 / 12, steps: 1249 / 1251, loss: 0.009411, speed: 4.85 step/s\n",
      "epoch time footprint: 0 hour 4 min 18 sec\n",
      "\n",
      "=====start training of 1 epochs=====\n",
      "epoch: 1 / 12, steps: 48 / 1251, loss: 0.010086, speed: 4.84 step/s\n",
      "epoch: 1 / 12, steps: 98 / 1251, loss: 0.014282, speed: 4.85 step/s\n",
      "epoch: 1 / 12, steps: 148 / 1251, loss: 0.007658, speed: 4.83 step/s\n",
      "epoch: 1 / 12, steps: 198 / 1251, loss: 0.006522, speed: 4.74 step/s\n",
      "epoch: 1 / 12, steps: 248 / 1251, loss: 0.008352, speed: 4.86 step/s\n",
      "epoch: 1 / 12, steps: 298 / 1251, loss: 0.006921, speed: 4.83 step/s\n",
      "epoch: 1 / 12, steps: 348 / 1251, loss: 0.006479, speed: 4.87 step/s\n",
      "epoch: 1 / 12, steps: 398 / 1251, loss: 0.008492, speed: 4.85 step/s\n",
      "epoch: 1 / 12, steps: 448 / 1251, loss: 0.007506, speed: 4.74 step/s\n",
      "epoch: 1 / 12, steps: 498 / 1251, loss: 0.006379, speed: 4.82 step/s\n",
      "epoch: 1 / 12, steps: 548 / 1251, loss: 0.005941, speed: 4.86 step/s\n",
      "epoch: 1 / 12, steps: 598 / 1251, loss: 0.006276, speed: 4.83 step/s\n",
      "epoch: 1 / 12, steps: 648 / 1251, loss: 0.004816, speed: 4.84 step/s\n",
      "epoch: 1 / 12, steps: 698 / 1251, loss: 0.005239, speed: 4.79 step/s\n",
      "epoch: 1 / 12, steps: 748 / 1251, loss: 0.004337, speed: 4.86 step/s\n",
      "epoch: 1 / 12, steps: 798 / 1251, loss: 0.009788, speed: 4.84 step/s\n",
      "epoch: 1 / 12, steps: 848 / 1251, loss: 0.004971, speed: 4.85 step/s\n",
      "epoch: 1 / 12, steps: 898 / 1251, loss: 0.005168, speed: 4.83 step/s\n",
      "epoch: 1 / 12, steps: 948 / 1251, loss: 0.005670, speed: 4.78 step/s\n",
      "epoch: 1 / 12, steps: 998 / 1251, loss: 0.007062, speed: 4.85 step/s\n",
      "epoch: 1 / 12, steps: 1048 / 1251, loss: 0.005934, speed: 4.86 step/s\n",
      "epoch: 1 / 12, steps: 1098 / 1251, loss: 0.004571, speed: 4.84 step/s\n",
      "epoch: 1 / 12, steps: 1148 / 1251, loss: 0.005045, speed: 4.85 step/s\n",
      "epoch: 1 / 12, steps: 1198 / 1251, loss: 0.004177, speed: 4.83 step/s\n",
      "epoch: 1 / 12, steps: 1248 / 1251, loss: 0.005479, speed: 4.79 step/s\n",
      "epoch time footprint: 0 hour 4 min 19 sec\n",
      "\n",
      "=====start training of 2 epochs=====\n",
      "epoch: 2 / 12, steps: 47 / 1251, loss: 0.003086, speed: 4.85 step/s\n",
      "epoch: 2 / 12, steps: 97 / 1251, loss: 0.003847, speed: 4.83 step/s\n",
      "epoch: 2 / 12, steps: 147 / 1251, loss: 0.003758, speed: 4.84 step/s\n",
      "epoch: 2 / 12, steps: 197 / 1251, loss: 0.003141, speed: 4.85 step/s\n",
      "epoch: 2 / 12, steps: 247 / 1251, loss: 0.003122, speed: 4.77 step/s\n",
      "epoch: 2 / 12, steps: 297 / 1251, loss: 0.004074, speed: 4.85 step/s\n",
      "epoch: 2 / 12, steps: 347 / 1251, loss: 0.003300, speed: 4.82 step/s\n",
      "epoch: 2 / 12, steps: 397 / 1251, loss: 0.004179, speed: 4.83 step/s\n",
      "epoch: 2 / 12, steps: 447 / 1251, loss: 0.003201, speed: 4.84 step/s\n",
      "epoch: 2 / 12, steps: 497 / 1251, loss: 0.003078, speed: 4.78 step/s\n",
      "epoch: 2 / 12, steps: 547 / 1251, loss: 0.006850, speed: 4.88 step/s\n",
      "epoch: 2 / 12, steps: 597 / 1251, loss: 0.003096, speed: 4.86 step/s\n",
      "epoch: 2 / 12, steps: 647 / 1251, loss: 0.002771, speed: 4.88 step/s\n",
      "epoch: 2 / 12, steps: 697 / 1251, loss: 0.001682, speed: 4.87 step/s\n",
      "epoch: 2 / 12, steps: 747 / 1251, loss: 0.002320, speed: 4.76 step/s\n",
      "epoch: 2 / 12, steps: 797 / 1251, loss: 0.002589, speed: 4.86 step/s\n",
      "epoch: 2 / 12, steps: 847 / 1251, loss: 0.004176, speed: 4.88 step/s\n",
      "epoch: 2 / 12, steps: 897 / 1251, loss: 0.004023, speed: 4.86 step/s\n",
      "epoch: 2 / 12, steps: 947 / 1251, loss: 0.004231, speed: 4.87 step/s\n",
      "epoch: 2 / 12, steps: 997 / 1251, loss: 0.003875, speed: 4.88 step/s\n",
      "epoch: 2 / 12, steps: 1047 / 1251, loss: 0.004105, speed: 4.82 step/s\n",
      "epoch: 2 / 12, steps: 1097 / 1251, loss: 0.003576, speed: 4.86 step/s\n",
      "epoch: 2 / 12, steps: 1147 / 1251, loss: 0.001913, speed: 4.88 step/s\n",
      "epoch: 2 / 12, steps: 1197 / 1251, loss: 0.002812, speed: 4.87 step/s\n",
      "epoch: 2 / 12, steps: 1247 / 1251, loss: 0.004521, speed: 4.88 step/s\n",
      "epoch time footprint: 0 hour 4 min 18 sec\n",
      "\n",
      "=====start training of 3 epochs=====\n",
      "epoch: 3 / 12, steps: 46 / 1251, loss: 0.002546, speed: 4.77 step/s\n",
      "epoch: 3 / 12, steps: 96 / 1251, loss: 0.001769, speed: 4.87 step/s\n",
      "epoch: 3 / 12, steps: 146 / 1251, loss: 0.002496, speed: 4.86 step/s\n",
      "epoch: 3 / 12, steps: 196 / 1251, loss: 0.002111, speed: 4.88 step/s\n",
      "epoch: 3 / 12, steps: 246 / 1251, loss: 0.002295, speed: 4.86 step/s\n",
      "epoch: 3 / 12, steps: 296 / 1251, loss: 0.002187, speed: 4.77 step/s\n",
      "epoch: 3 / 12, steps: 346 / 1251, loss: 0.002142, speed: 4.87 step/s\n",
      "epoch: 3 / 12, steps: 396 / 1251, loss: 0.003076, speed: 4.87 step/s\n",
      "epoch: 3 / 12, steps: 446 / 1251, loss: 0.002587, speed: 4.84 step/s\n",
      "epoch: 3 / 12, steps: 496 / 1251, loss: 0.001759, speed: 4.87 step/s\n",
      "epoch: 3 / 12, steps: 546 / 1251, loss: 0.001466, speed: 4.78 step/s\n",
      "epoch: 3 / 12, steps: 596 / 1251, loss: 0.002037, speed: 4.87 step/s\n",
      "epoch: 3 / 12, steps: 646 / 1251, loss: 0.000987, speed: 4.83 step/s\n",
      "epoch: 3 / 12, steps: 696 / 1251, loss: 0.002393, speed: 4.87 step/s\n",
      "epoch: 3 / 12, steps: 746 / 1251, loss: 0.001866, speed: 4.86 step/s\n",
      "epoch: 3 / 12, steps: 796 / 1251, loss: 0.001952, speed: 4.78 step/s\n",
      "epoch: 3 / 12, steps: 846 / 1251, loss: 0.002156, speed: 4.87 step/s\n",
      "epoch: 3 / 12, steps: 896 / 1251, loss: 0.002108, speed: 4.82 step/s\n",
      "epoch: 3 / 12, steps: 946 / 1251, loss: 0.002913, speed: 4.89 step/s\n",
      "epoch: 3 / 12, steps: 996 / 1251, loss: 0.001724, speed: 4.86 step/s\n",
      "epoch: 3 / 12, steps: 1046 / 1251, loss: 0.002425, speed: 4.77 step/s\n",
      "epoch: 3 / 12, steps: 1096 / 1251, loss: 0.003140, speed: 4.85 step/s\n",
      "epoch: 3 / 12, steps: 1146 / 1251, loss: 0.001863, speed: 4.87 step/s\n",
      "epoch: 3 / 12, steps: 1196 / 1251, loss: 0.003513, speed: 4.87 step/s\n",
      "epoch: 3 / 12, steps: 1246 / 1251, loss: 0.002118, speed: 4.86 step/s\n",
      "epoch time footprint: 0 hour 4 min 18 sec\n",
      "\n",
      "=====start training of 4 epochs=====\n",
      "epoch: 4 / 12, steps: 45 / 1251, loss: 0.001524, speed: 4.79 step/s\n",
      "epoch: 4 / 12, steps: 95 / 1251, loss: 0.002267, speed: 4.86 step/s\n",
      "epoch: 4 / 12, steps: 145 / 1251, loss: 0.001563, speed: 4.86 step/s\n",
      "epoch: 4 / 12, steps: 195 / 1251, loss: 0.000946, speed: 4.86 step/s\n",
      "epoch: 4 / 12, steps: 245 / 1251, loss: 0.001125, speed: 4.87 step/s\n",
      "epoch: 4 / 12, steps: 295 / 1251, loss: 0.001669, speed: 4.88 step/s\n",
      "epoch: 4 / 12, steps: 345 / 1251, loss: 0.001297, speed: 4.78 step/s\n",
      "epoch: 4 / 12, steps: 395 / 1251, loss: 0.002366, speed: 4.85 step/s\n",
      "epoch: 4 / 12, steps: 445 / 1251, loss: 0.001103, speed: 4.86 step/s\n",
      "epoch: 4 / 12, steps: 495 / 1251, loss: 0.001149, speed: 4.84 step/s\n",
      "epoch: 4 / 12, steps: 545 / 1251, loss: 0.001472, speed: 4.87 step/s\n",
      "epoch: 4 / 12, steps: 595 / 1251, loss: 0.000763, speed: 4.81 step/s\n",
      "epoch: 4 / 12, steps: 645 / 1251, loss: 0.001261, speed: 4.87 step/s\n",
      "epoch: 4 / 12, steps: 695 / 1251, loss: 0.000988, speed: 4.87 step/s\n",
      "epoch: 4 / 12, steps: 745 / 1251, loss: 0.001037, speed: 4.81 step/s\n",
      "epoch: 4 / 12, steps: 795 / 1251, loss: 0.001120, speed: 4.87 step/s\n",
      "epoch: 4 / 12, steps: 845 / 1251, loss: 0.001666, speed: 4.75 step/s\n",
      "epoch: 4 / 12, steps: 895 / 1251, loss: 0.002752, speed: 4.87 step/s\n",
      "epoch: 4 / 12, steps: 945 / 1251, loss: 0.001529, speed: 4.87 step/s\n",
      "epoch: 4 / 12, steps: 995 / 1251, loss: 0.000897, speed: 4.86 step/s\n",
      "epoch: 4 / 12, steps: 1045 / 1251, loss: 0.002524, speed: 4.83 step/s\n",
      "epoch: 4 / 12, steps: 1095 / 1251, loss: 0.000833, speed: 4.80 step/s\n",
      "epoch: 4 / 12, steps: 1145 / 1251, loss: 0.001842, speed: 4.87 step/s\n",
      "epoch: 4 / 12, steps: 1195 / 1251, loss: 0.001134, speed: 4.85 step/s\n",
      "epoch: 4 / 12, steps: 1245 / 1251, loss: 0.000773, speed: 4.86 step/s\n",
      "epoch time footprint: 0 hour 4 min 18 sec\n",
      "\n",
      "=====start training of 5 epochs=====\n",
      "epoch: 5 / 12, steps: 44 / 1251, loss: 0.001074, speed: 4.87 step/s\n",
      "epoch: 5 / 12, steps: 94 / 1251, loss: 0.001037, speed: 4.80 step/s\n",
      "epoch: 5 / 12, steps: 144 / 1251, loss: 0.000879, speed: 4.83 step/s\n",
      "epoch: 5 / 12, steps: 194 / 1251, loss: 0.000472, speed: 4.87 step/s\n",
      "epoch: 5 / 12, steps: 244 / 1251, loss: 0.000549, speed: 4.86 step/s\n",
      "epoch: 5 / 12, steps: 294 / 1251, loss: 0.000719, speed: 4.87 step/s\n",
      "epoch: 5 / 12, steps: 344 / 1251, loss: 0.000737, speed: 4.78 step/s\n",
      "epoch: 5 / 12, steps: 394 / 1251, loss: 0.000826, speed: 4.86 step/s\n",
      "epoch: 5 / 12, steps: 444 / 1251, loss: 0.000299, speed: 4.87 step/s\n",
      "epoch: 5 / 12, steps: 494 / 1251, loss: 0.000577, speed: 4.86 step/s\n",
      "epoch: 5 / 12, steps: 544 / 1251, loss: 0.000450, speed: 4.87 step/s\n",
      "epoch: 5 / 12, steps: 594 / 1251, loss: 0.001230, speed: 4.81 step/s\n",
      "epoch: 5 / 12, steps: 644 / 1251, loss: 0.001057, speed: 4.84 step/s\n",
      "epoch: 5 / 12, steps: 694 / 1251, loss: 0.000937, speed: 4.85 step/s\n",
      "epoch: 5 / 12, steps: 744 / 1251, loss: 0.000670, speed: 4.87 step/s\n",
      "epoch: 5 / 12, steps: 794 / 1251, loss: 0.000655, speed: 4.87 step/s\n",
      "epoch: 5 / 12, steps: 844 / 1251, loss: 0.000720, speed: 4.82 step/s\n",
      "epoch: 5 / 12, steps: 894 / 1251, loss: 0.000572, speed: 4.85 step/s\n",
      "epoch: 5 / 12, steps: 944 / 1251, loss: 0.000803, speed: 4.84 step/s\n",
      "epoch: 5 / 12, steps: 994 / 1251, loss: 0.001019, speed: 4.86 step/s\n",
      "epoch: 5 / 12, steps: 1044 / 1251, loss: 0.000541, speed: 4.86 step/s\n",
      "epoch: 5 / 12, steps: 1094 / 1251, loss: 0.000817, speed: 4.86 step/s\n",
      "epoch: 5 / 12, steps: 1144 / 1251, loss: 0.002423, speed: 4.81 step/s\n",
      "epoch: 5 / 12, steps: 1194 / 1251, loss: 0.001313, speed: 4.87 step/s\n",
      "epoch: 5 / 12, steps: 1244 / 1251, loss: 0.000705, speed: 4.87 step/s\n",
      "epoch time footprint: 0 hour 4 min 17 sec\n",
      "\n",
      "=====start training of 6 epochs=====\n",
      "epoch: 6 / 12, steps: 43 / 1251, loss: 0.000582, speed: 4.83 step/s\n",
      "epoch: 6 / 12, steps: 93 / 1251, loss: 0.000381, speed: 4.88 step/s\n",
      "epoch: 6 / 12, steps: 143 / 1251, loss: 0.001690, speed: 4.79 step/s\n",
      "epoch: 6 / 12, steps: 193 / 1251, loss: 0.000497, speed: 4.84 step/s\n",
      "epoch: 6 / 12, steps: 243 / 1251, loss: 0.001084, speed: 4.87 step/s\n",
      "epoch: 6 / 12, steps: 293 / 1251, loss: 0.001188, speed: 4.88 step/s\n",
      "epoch: 6 / 12, steps: 343 / 1251, loss: 0.000448, speed: 4.88 step/s\n",
      "epoch: 6 / 12, steps: 393 / 1251, loss: 0.000320, speed: 4.78 step/s\n",
      "epoch: 6 / 12, steps: 443 / 1251, loss: 0.000395, speed: 4.87 step/s\n",
      "epoch: 6 / 12, steps: 493 / 1251, loss: 0.000337, speed: 4.89 step/s\n",
      "epoch: 6 / 12, steps: 543 / 1251, loss: 0.001316, speed: 4.87 step/s\n",
      "epoch: 6 / 12, steps: 593 / 1251, loss: 0.000606, speed: 4.87 step/s\n",
      "epoch: 6 / 12, steps: 643 / 1251, loss: 0.000205, speed: 4.78 step/s\n",
      "epoch: 6 / 12, steps: 693 / 1251, loss: 0.000476, speed: 4.87 step/s\n",
      "epoch: 6 / 12, steps: 743 / 1251, loss: 0.000863, speed: 4.86 step/s\n",
      "epoch: 6 / 12, steps: 793 / 1251, loss: 0.000265, speed: 4.84 step/s\n",
      "epoch: 6 / 12, steps: 843 / 1251, loss: 0.001395, speed: 4.87 step/s\n",
      "epoch: 6 / 12, steps: 893 / 1251, loss: 0.000250, speed: 4.79 step/s\n",
      "epoch: 6 / 12, steps: 943 / 1251, loss: 0.000783, speed: 4.89 step/s\n",
      "epoch: 6 / 12, steps: 993 / 1251, loss: 0.000569, speed: 4.86 step/s\n",
      "epoch: 6 / 12, steps: 1043 / 1251, loss: 0.000791, speed: 4.88 step/s\n",
      "epoch: 6 / 12, steps: 1093 / 1251, loss: 0.000232, speed: 4.88 step/s\n",
      "epoch: 6 / 12, steps: 1143 / 1251, loss: 0.001450, speed: 4.75 step/s\n",
      "epoch: 6 / 12, steps: 1193 / 1251, loss: 0.000375, speed: 4.86 step/s\n",
      "epoch: 6 / 12, steps: 1243 / 1251, loss: 0.000482, speed: 4.88 step/s\n",
      "epoch time footprint: 0 hour 4 min 17 sec\n",
      "\n",
      "=====start training of 7 epochs=====\n",
      "epoch: 7 / 12, steps: 42 / 1251, loss: 0.001342, speed: 4.87 step/s\n",
      "epoch: 7 / 12, steps: 92 / 1251, loss: 0.000561, speed: 4.87 step/s\n",
      "epoch: 7 / 12, steps: 142 / 1251, loss: 0.000586, speed: 4.85 step/s\n",
      "epoch: 7 / 12, steps: 192 / 1251, loss: 0.000518, speed: 4.81 step/s\n",
      "epoch: 7 / 12, steps: 242 / 1251, loss: 0.001223, speed: 4.82 step/s\n",
      "epoch: 7 / 12, steps: 292 / 1251, loss: 0.000210, speed: 4.82 step/s\n",
      "epoch: 7 / 12, steps: 342 / 1251, loss: 0.000426, speed: 4.78 step/s\n",
      "epoch: 7 / 12, steps: 392 / 1251, loss: 0.000314, speed: 4.83 step/s\n",
      "epoch: 7 / 12, steps: 442 / 1251, loss: 0.000341, speed: 4.78 step/s\n",
      "epoch: 7 / 12, steps: 492 / 1251, loss: 0.000198, speed: 4.87 step/s\n",
      "epoch: 7 / 12, steps: 542 / 1251, loss: 0.000582, speed: 4.87 step/s\n",
      "epoch: 7 / 12, steps: 592 / 1251, loss: 0.000424, speed: 4.88 step/s\n",
      "epoch: 7 / 12, steps: 642 / 1251, loss: 0.000572, speed: 4.85 step/s\n",
      "epoch: 7 / 12, steps: 692 / 1251, loss: 0.001212, speed: 4.78 step/s\n",
      "epoch: 7 / 12, steps: 742 / 1251, loss: 0.000116, speed: 4.86 step/s\n",
      "epoch: 7 / 12, steps: 792 / 1251, loss: 0.001170, speed: 4.89 step/s\n",
      "epoch: 7 / 12, steps: 842 / 1251, loss: 0.000278, speed: 4.88 step/s\n",
      "epoch: 7 / 12, steps: 892 / 1251, loss: 0.000293, speed: 4.88 step/s\n",
      "epoch: 7 / 12, steps: 942 / 1251, loss: 0.000302, speed: 4.78 step/s\n",
      "epoch: 7 / 12, steps: 992 / 1251, loss: 0.000411, speed: 4.89 step/s\n",
      "epoch: 7 / 12, steps: 1042 / 1251, loss: 0.000905, speed: 4.85 step/s\n",
      "epoch: 7 / 12, steps: 1092 / 1251, loss: 0.000142, speed: 4.87 step/s\n",
      "epoch: 7 / 12, steps: 1142 / 1251, loss: 0.000219, speed: 4.88 step/s\n",
      "epoch: 7 / 12, steps: 1192 / 1251, loss: 0.000519, speed: 4.78 step/s\n",
      "epoch: 7 / 12, steps: 1242 / 1251, loss: 0.000337, speed: 4.85 step/s\n",
      "\n",
      "=====start evaluating ckpt of 10000 steps=====\n",
      "\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]\n",
      "  2%|▏         | 2/125 [00:00<00:08, 14.34it/s]\n",
      "  3%|▎         | 4/125 [00:00<00:08, 14.52it/s]\n",
      "  5%|▍         | 6/125 [00:00<00:08, 14.67it/s]\n",
      "  6%|▋         | 8/125 [00:00<00:07, 14.73it/s]\n",
      "  8%|▊         | 10/125 [00:00<00:07, 14.80it/s]\n",
      " 10%|▉         | 12/125 [00:00<00:07, 14.87it/s]\n",
      " 11%|█         | 14/125 [00:00<00:07, 14.84it/s]\n",
      " 13%|█▎        | 16/125 [00:01<00:07, 14.92it/s]\n",
      " 14%|█▍        | 18/125 [00:01<00:07, 14.92it/s]\n",
      " 16%|█▌        | 20/125 [00:01<00:07, 14.91it/s]\n",
      " 18%|█▊        | 22/125 [00:01<00:06, 15.01it/s]\n",
      " 19%|█▉        | 24/125 [00:01<00:06, 15.04it/s]\n",
      " 21%|██        | 26/125 [00:01<00:06, 15.06it/s]\n",
      " 22%|██▏       | 28/125 [00:01<00:06, 15.03it/s]\n",
      " 24%|██▍       | 30/125 [00:02<00:06, 15.03it/s]\n",
      " 26%|██▌       | 32/125 [00:02<00:06, 15.07it/s]\n",
      " 27%|██▋       | 34/125 [00:02<00:06, 15.08it/s]\n",
      " 29%|██▉       | 36/125 [00:02<00:05, 15.02it/s]\n",
      " 30%|███       | 38/125 [00:02<00:05, 15.02it/s]\n",
      " 32%|███▏      | 40/125 [00:02<00:05, 15.06it/s]\n",
      " 34%|███▎      | 42/125 [00:02<00:05, 15.09it/s]\n",
      " 35%|███▌      | 44/125 [00:02<00:05, 15.08it/s]\n",
      " 37%|███▋      | 46/125 [00:03<00:05, 15.15it/s]\n",
      " 38%|███▊      | 48/125 [00:03<00:05, 15.07it/s]\n",
      " 40%|████      | 50/125 [00:03<00:04, 15.05it/s]\n",
      " 42%|████▏     | 52/125 [00:03<00:04, 15.01it/s]\n",
      " 43%|████▎     | 54/125 [00:03<00:04, 14.96it/s]\n",
      " 45%|████▍     | 56/125 [00:03<00:04, 14.97it/s]\n",
      " 46%|████▋     | 58/125 [00:03<00:04, 14.96it/s]\n",
      " 48%|████▊     | 60/125 [00:04<00:04, 14.94it/s]\n",
      " 50%|████▉     | 62/125 [00:04<00:04, 14.93it/s]\n",
      " 51%|█████     | 64/125 [00:04<00:04, 14.98it/s]\n",
      " 53%|█████▎    | 66/125 [00:04<00:03, 14.89it/s]\n",
      " 54%|█████▍    | 68/125 [00:04<00:03, 14.98it/s]\n",
      " 56%|█████▌    | 70/125 [00:04<00:03, 15.08it/s]\n",
      " 58%|█████▊    | 72/125 [00:04<00:03, 15.10it/s]\n",
      " 59%|█████▉    | 74/125 [00:04<00:03, 15.06it/s]\n",
      " 61%|██████    | 76/125 [00:05<00:03, 15.07it/s]\n",
      " 62%|██████▏   | 78/125 [00:05<00:03, 15.07it/s]\n",
      " 64%|██████▍   | 80/125 [00:05<00:02, 15.13it/s]\n",
      " 66%|██████▌   | 82/125 [00:05<00:02, 15.07it/s]\n",
      " 67%|██████▋   | 84/125 [00:05<00:02, 15.10it/s]\n",
      " 69%|██████▉   | 86/125 [00:05<00:02, 15.11it/s]\n",
      " 70%|███████   | 88/125 [00:05<00:02, 15.17it/s]\n",
      " 72%|███████▏  | 90/125 [00:05<00:02, 15.11it/s]\n",
      " 74%|███████▎  | 92/125 [00:06<00:02, 15.06it/s]\n",
      " 75%|███████▌  | 94/125 [00:06<00:02, 15.07it/s]\n",
      " 77%|███████▋  | 96/125 [00:06<00:01, 15.08it/s]\n",
      " 78%|███████▊  | 98/125 [00:06<00:01, 15.09it/s]\n",
      " 80%|████████  | 100/125 [00:06<00:01, 15.14it/s]\n",
      " 82%|████████▏ | 102/125 [00:06<00:01, 15.13it/s]\n",
      " 83%|████████▎ | 104/125 [00:06<00:01, 15.15it/s]\n",
      " 85%|████████▍ | 106/125 [00:07<00:01, 15.09it/s]\n",
      " 86%|████████▋ | 108/125 [00:07<00:01, 15.08it/s]\n",
      " 88%|████████▊ | 110/125 [00:07<00:00, 15.04it/s]\n",
      " 90%|████████▉ | 112/125 [00:07<00:00, 15.07it/s]\n",
      " 91%|█████████ | 114/125 [00:07<00:00, 15.04it/s]\n",
      " 93%|█████████▎| 116/125 [00:07<00:00, 15.09it/s]\n",
      " 94%|█████████▍| 118/125 [00:07<00:00, 15.08it/s]\n",
      " 96%|█████████▌| 120/125 [00:07<00:00, 15.07it/s]\n",
      " 98%|█████████▊| 122/125 [00:08<00:00, 15.09it/s]\n",
      " 99%|█████████▉| 124/125 [00:08<00:00, 15.11it/s]\n",
      "100%|██████████| 125/125 [00:08<00:00, 15.04it/s]\n",
      "eval loss: 0.003952\n",
      "correct spo num = 1224.0\n",
      "submitted spo num = 1947.0\n",
      "golden set spo num = 1771.0\n",
      "submitted recall spo num = 1224.0\n",
      "precision: 62.87\t recall: 69.11\t f1: 65.84\t\n",
      "saving checkpoing model_10000.pdparams to ./checkpoints \n",
      "epoch time footprint: 0 hour 4 min 43 sec\n",
      "\n",
      "=====start training of 8 epochs=====\n",
      "epoch: 8 / 12, steps: 41 / 1251, loss: 0.000423, speed: 1.40 step/s\n",
      "epoch: 8 / 12, steps: 91 / 1251, loss: 0.000483, speed: 4.76 step/s\n",
      "epoch: 8 / 12, steps: 141 / 1251, loss: 0.000367, speed: 4.87 step/s\n",
      "epoch: 8 / 12, steps: 191 / 1251, loss: 0.000347, speed: 4.88 step/s\n",
      "epoch: 8 / 12, steps: 241 / 1251, loss: 0.000305, speed: 4.88 step/s\n",
      "epoch: 8 / 12, steps: 291 / 1251, loss: 0.001002, speed: 4.87 step/s\n",
      "epoch: 8 / 12, steps: 341 / 1251, loss: 0.000323, speed: 4.77 step/s\n",
      "epoch: 8 / 12, steps: 391 / 1251, loss: 0.000120, speed: 4.89 step/s\n",
      "epoch: 8 / 12, steps: 441 / 1251, loss: 0.000172, speed: 4.85 step/s\n",
      "epoch: 8 / 12, steps: 491 / 1251, loss: 0.000209, speed: 4.87 step/s\n",
      "epoch: 8 / 12, steps: 541 / 1251, loss: 0.000765, speed: 4.88 step/s\n",
      "epoch: 8 / 12, steps: 591 / 1251, loss: 0.000454, speed: 4.81 step/s\n",
      "epoch: 8 / 12, steps: 641 / 1251, loss: 0.000175, speed: 4.90 step/s\n",
      "epoch: 8 / 12, steps: 691 / 1251, loss: 0.000082, speed: 4.90 step/s\n",
      "epoch: 8 / 12, steps: 741 / 1251, loss: 0.000192, speed: 4.89 step/s\n",
      "epoch: 8 / 12, steps: 791 / 1251, loss: 0.000385, speed: 4.89 step/s\n",
      "epoch: 8 / 12, steps: 841 / 1251, loss: 0.000736, speed: 4.80 step/s\n",
      "epoch: 8 / 12, steps: 891 / 1251, loss: 0.000876, speed: 4.89 step/s\n",
      "epoch: 8 / 12, steps: 941 / 1251, loss: 0.000388, speed: 4.90 step/s\n",
      "epoch: 8 / 12, steps: 991 / 1251, loss: 0.000321, speed: 4.90 step/s\n",
      "epoch: 8 / 12, steps: 1041 / 1251, loss: 0.000428, speed: 4.88 step/s\n",
      "epoch: 8 / 12, steps: 1091 / 1251, loss: 0.000869, speed: 4.78 step/s\n",
      "epoch: 8 / 12, steps: 1141 / 1251, loss: 0.000244, speed: 4.89 step/s\n",
      "epoch: 8 / 12, steps: 1191 / 1251, loss: 0.001010, speed: 4.85 step/s\n",
      "epoch: 8 / 12, steps: 1241 / 1251, loss: 0.001118, speed: 4.89 step/s\n",
      "epoch time footprint: 0 hour 4 min 17 sec\n",
      "\n",
      "=====start training of 9 epochs=====\n",
      "epoch: 9 / 12, steps: 40 / 1251, loss: 0.000463, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 90 / 1251, loss: 0.000185, speed: 4.82 step/s\n",
      "epoch: 9 / 12, steps: 140 / 1251, loss: 0.000612, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 190 / 1251, loss: 0.000364, speed: 4.86 step/s\n",
      "epoch: 9 / 12, steps: 240 / 1251, loss: 0.000422, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 290 / 1251, loss: 0.000245, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 340 / 1251, loss: 0.000073, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 390 / 1251, loss: 0.000251, speed: 4.80 step/s\n",
      "epoch: 9 / 12, steps: 440 / 1251, loss: 0.001132, speed: 4.88 step/s\n",
      "epoch: 9 / 12, steps: 490 / 1251, loss: 0.000154, speed: 4.86 step/s\n",
      "epoch: 9 / 12, steps: 540 / 1251, loss: 0.000378, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 590 / 1251, loss: 0.000417, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 640 / 1251, loss: 0.000204, speed: 4.81 step/s\n",
      "epoch: 9 / 12, steps: 690 / 1251, loss: 0.000599, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 740 / 1251, loss: 0.000123, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 790 / 1251, loss: 0.000472, speed: 4.88 step/s\n",
      "epoch: 9 / 12, steps: 840 / 1251, loss: 0.000162, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 890 / 1251, loss: 0.000188, speed: 4.81 step/s\n",
      "epoch: 9 / 12, steps: 940 / 1251, loss: 0.000288, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 990 / 1251, loss: 0.000208, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 1040 / 1251, loss: 0.000401, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 1090 / 1251, loss: 0.000390, speed: 4.88 step/s\n",
      "epoch: 9 / 12, steps: 1140 / 1251, loss: 0.000061, speed: 4.76 step/s\n",
      "epoch: 9 / 12, steps: 1190 / 1251, loss: 0.000408, speed: 4.89 step/s\n",
      "epoch: 9 / 12, steps: 1240 / 1251, loss: 0.000308, speed: 4.89 step/s\n",
      "epoch time footprint: 0 hour 4 min 16 sec\n",
      "\n",
      "=====start training of 10 epochs=====\n",
      "epoch: 10 / 12, steps: 39 / 1251, loss: 0.000321, speed: 4.89 step/s\n",
      "epoch: 10 / 12, steps: 89 / 1251, loss: 0.000261, speed: 4.89 step/s\n",
      "epoch: 10 / 12, steps: 139 / 1251, loss: 0.001197, speed: 4.82 step/s\n",
      "epoch: 10 / 12, steps: 189 / 1251, loss: 0.000322, speed: 4.89 step/s\n",
      "epoch: 10 / 12, steps: 239 / 1251, loss: 0.000139, speed: 4.90 step/s\n",
      "epoch: 10 / 12, steps: 289 / 1251, loss: 0.000208, speed: 4.89 step/s\n",
      "epoch: 10 / 12, steps: 339 / 1251, loss: 0.000640, speed: 4.88 step/s\n",
      "epoch: 10 / 12, steps: 389 / 1251, loss: 0.000185, speed: 4.77 step/s\n",
      "epoch: 10 / 12, steps: 439 / 1251, loss: 0.000103, speed: 4.88 step/s\n",
      "epoch: 10 / 12, steps: 489 / 1251, loss: 0.000105, speed: 4.88 step/s\n",
      "epoch: 10 / 12, steps: 539 / 1251, loss: 0.000249, speed: 4.88 step/s\n",
      "epoch: 10 / 12, steps: 589 / 1251, loss: 0.000296, speed: 4.89 step/s\n",
      "epoch: 10 / 12, steps: 639 / 1251, loss: 0.000074, speed: 4.90 step/s\n",
      "epoch: 10 / 12, steps: 689 / 1251, loss: 0.000216, speed: 4.79 step/s\n",
      "epoch: 10 / 12, steps: 739 / 1251, loss: 0.000226, speed: 4.87 step/s\n",
      "epoch: 10 / 12, steps: 789 / 1251, loss: 0.000086, speed: 4.88 step/s\n",
      "epoch: 10 / 12, steps: 839 / 1251, loss: 0.000699, speed: 4.89 step/s\n",
      "epoch: 10 / 12, steps: 889 / 1251, loss: 0.000876, speed: 4.85 step/s\n",
      "epoch: 10 / 12, steps: 939 / 1251, loss: 0.000243, speed: 4.78 step/s\n",
      "epoch: 10 / 12, steps: 989 / 1251, loss: 0.000182, speed: 4.89 step/s\n",
      "epoch: 10 / 12, steps: 1039 / 1251, loss: 0.000095, speed: 4.89 step/s\n",
      "epoch: 10 / 12, steps: 1089 / 1251, loss: 0.000071, speed: 4.89 step/s\n",
      "epoch: 10 / 12, steps: 1139 / 1251, loss: 0.000091, speed: 4.87 step/s\n",
      "epoch: 10 / 12, steps: 1189 / 1251, loss: 0.000140, speed: 4.89 step/s\n",
      "epoch: 10 / 12, steps: 1239 / 1251, loss: 0.000145, speed: 4.89 step/s\n",
      "epoch time footprint: 0 hour 4 min 16 sec\n",
      "\n",
      "=====start training of 11 epochs=====\n",
      "epoch: 11 / 12, steps: 38 / 1251, loss: 0.000585, speed: 4.89 step/s\n",
      "epoch: 11 / 12, steps: 88 / 1251, loss: 0.000559, speed: 4.89 step/s\n",
      "epoch: 11 / 12, steps: 138 / 1251, loss: 0.000209, speed: 4.82 step/s\n",
      "epoch: 11 / 12, steps: 188 / 1251, loss: 0.000162, speed: 4.88 step/s\n",
      "epoch: 11 / 12, steps: 238 / 1251, loss: 0.000231, speed: 4.89 step/s\n",
      "epoch: 11 / 12, steps: 288 / 1251, loss: 0.000201, speed: 4.86 step/s\n",
      "epoch: 11 / 12, steps: 338 / 1251, loss: 0.000128, speed: 4.88 step/s\n",
      "epoch: 11 / 12, steps: 388 / 1251, loss: 0.000057, speed: 4.81 step/s\n",
      "epoch: 11 / 12, steps: 438 / 1251, loss: 0.000128, speed: 4.89 step/s\n",
      "epoch: 11 / 12, steps: 488 / 1251, loss: 0.000399, speed: 4.89 step/s\n",
      "epoch: 11 / 12, steps: 538 / 1251, loss: 0.000073, speed: 4.90 step/s\n",
      "epoch: 11 / 12, steps: 588 / 1251, loss: 0.000181, speed: 4.84 step/s\n",
      "epoch: 11 / 12, steps: 638 / 1251, loss: 0.000224, speed: 4.78 step/s\n",
      "epoch: 11 / 12, steps: 688 / 1251, loss: 0.000171, speed: 4.88 step/s\n",
      "epoch: 11 / 12, steps: 738 / 1251, loss: 0.000244, speed: 4.84 step/s\n",
      "epoch: 11 / 12, steps: 788 / 1251, loss: 0.000152, speed: 4.85 step/s\n",
      "epoch: 11 / 12, steps: 838 / 1251, loss: 0.000168, speed: 4.87 step/s\n",
      "epoch: 11 / 12, steps: 888 / 1251, loss: 0.000235, speed: 4.86 step/s\n",
      "epoch: 11 / 12, steps: 938 / 1251, loss: 0.000747, speed: 4.75 step/s\n",
      "epoch: 11 / 12, steps: 988 / 1251, loss: 0.000498, speed: 4.84 step/s\n",
      "epoch: 11 / 12, steps: 1038 / 1251, loss: 0.000155, speed: 4.85 step/s\n",
      "epoch: 11 / 12, steps: 1088 / 1251, loss: 0.000086, speed: 4.83 step/s\n",
      "epoch: 11 / 12, steps: 1138 / 1251, loss: 0.000087, speed: 4.86 step/s\n",
      "epoch: 11 / 12, steps: 1188 / 1251, loss: 0.000447, speed: 4.80 step/s\n",
      "epoch: 11 / 12, steps: 1238 / 1251, loss: 0.000061, speed: 4.87 step/s\n",
      "epoch time footprint: 0 hour 4 min 17 sec\n",
      "\n",
      "=====start evaluating last ckpt of 15012 steps=====\n",
      "\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]\n",
      "  2%|▏         | 2/125 [00:00<00:08, 14.44it/s]\n",
      "  3%|▎         | 4/125 [00:00<00:08, 14.63it/s]\n",
      "  5%|▍         | 6/125 [00:00<00:08, 14.78it/s]\n",
      "  6%|▋         | 8/125 [00:00<00:07, 14.84it/s]\n",
      "  8%|▊         | 10/125 [00:00<00:07, 14.93it/s]\n",
      " 10%|▉         | 12/125 [00:00<00:07, 14.99it/s]\n",
      " 11%|█         | 14/125 [00:00<00:07, 14.95it/s]\n",
      " 13%|█▎        | 16/125 [00:01<00:07, 15.05it/s]\n",
      " 14%|█▍        | 18/125 [00:01<00:07, 15.05it/s]\n",
      " 16%|█▌        | 20/125 [00:01<00:06, 15.05it/s]\n",
      " 18%|█▊        | 22/125 [00:01<00:06, 15.10it/s]\n",
      " 19%|█▉        | 24/125 [00:01<00:06, 15.11it/s]\n",
      " 21%|██        | 26/125 [00:01<00:06, 15.11it/s]\n",
      " 22%|██▏       | 28/125 [00:01<00:06, 15.09it/s]\n",
      " 24%|██▍       | 30/125 [00:01<00:06, 15.09it/s]\n",
      " 26%|██▌       | 32/125 [00:02<00:06, 15.15it/s]\n",
      " 27%|██▋       | 34/125 [00:02<00:05, 15.18it/s]\n",
      " 29%|██▉       | 36/125 [00:02<00:05, 15.14it/s]\n",
      " 30%|███       | 38/125 [00:02<00:05, 15.13it/s]\n",
      " 32%|███▏      | 40/125 [00:02<00:05, 15.16it/s]\n",
      " 34%|███▎      | 42/125 [00:02<00:05, 15.19it/s]\n",
      " 35%|███▌      | 44/125 [00:02<00:05, 15.17it/s]\n",
      " 37%|███▋      | 46/125 [00:03<00:05, 15.23it/s]\n",
      " 38%|███▊      | 48/125 [00:03<00:05, 15.19it/s]\n",
      " 40%|████      | 50/125 [00:03<00:04, 15.18it/s]\n",
      " 42%|████▏     | 52/125 [00:03<00:04, 15.10it/s]\n",
      " 43%|████▎     | 54/125 [00:03<00:04, 15.05it/s]\n",
      " 45%|████▍     | 56/125 [00:03<00:04, 15.07it/s]\n",
      " 46%|████▋     | 58/125 [00:03<00:04, 15.07it/s]\n",
      " 48%|████▊     | 60/125 [00:03<00:04, 15.07it/s]\n",
      " 50%|████▉     | 62/125 [00:04<00:04, 15.05it/s]\n",
      " 51%|█████     | 64/125 [00:04<00:04, 15.07it/s]\n",
      " 53%|█████▎    | 66/125 [00:04<00:03, 15.01it/s]\n",
      " 54%|█████▍    | 68/125 [00:04<00:03, 15.06it/s]\n",
      " 56%|█████▌    | 70/125 [00:04<00:03, 15.10it/s]\n",
      " 58%|█████▊    | 72/125 [00:04<00:03, 15.05it/s]\n",
      " 59%|█████▉    | 74/125 [00:04<00:03, 15.03it/s]\n",
      " 61%|██████    | 76/125 [00:05<00:03, 15.02it/s]\n",
      " 62%|██████▏   | 78/125 [00:05<00:03, 15.02it/s]\n",
      " 64%|██████▍   | 80/125 [00:05<00:02, 15.06it/s]\n",
      " 66%|██████▌   | 82/125 [00:05<00:02, 14.99it/s]\n",
      " 67%|██████▋   | 84/125 [00:05<00:02, 15.05it/s]\n",
      " 69%|██████▉   | 86/125 [00:05<00:02, 15.09it/s]\n",
      " 70%|███████   | 88/125 [00:05<00:02, 15.14it/s]\n",
      " 72%|███████▏  | 90/125 [00:05<00:02, 15.10it/s]\n",
      " 74%|███████▎  | 92/125 [00:06<00:02, 15.04it/s]\n",
      " 75%|███████▌  | 94/125 [00:06<00:02, 15.05it/s]\n",
      " 77%|███████▋  | 96/125 [00:06<00:01, 15.06it/s]\n",
      " 78%|███████▊  | 98/125 [00:06<00:01, 15.07it/s]\n",
      " 80%|████████  | 100/125 [00:06<00:01, 15.10it/s]\n",
      " 82%|████████▏ | 102/125 [00:06<00:01, 15.10it/s]\n",
      " 83%|████████▎ | 104/125 [00:06<00:01, 15.11it/s]\n",
      " 85%|████████▍ | 106/125 [00:07<00:01, 15.05it/s]\n",
      " 86%|████████▋ | 108/125 [00:07<00:01, 15.00it/s]\n",
      " 88%|████████▊ | 110/125 [00:07<00:01, 14.98it/s]\n",
      " 90%|████████▉ | 112/125 [00:07<00:00, 15.04it/s]\n",
      " 91%|█████████ | 114/125 [00:07<00:00, 15.02it/s]\n",
      " 93%|█████████▎| 116/125 [00:07<00:00, 15.08it/s]\n",
      " 94%|█████████▍| 118/125 [00:07<00:00, 15.07it/s]\n",
      " 96%|█████████▌| 120/125 [00:07<00:00, 15.03it/s]\n",
      " 98%|█████████▊| 122/125 [00:08<00:00, 15.07it/s]\n",
      " 99%|█████████▉| 124/125 [00:08<00:00, 15.07it/s]\n",
      "100%|██████████| 125/125 [00:08<00:00, 15.07it/s]\n",
      "eval loss: 0.004724\n",
      "correct spo num = 1263.0\n",
      "submitted spo num = 2008.0\n",
      "golden set spo num = 1771.0\n",
      "submitted recall spo num = 1263.0\n",
      "precision: 62.90\t recall: 71.32\t f1: 66.84\t\n",
      "\n",
      "=====training complete=====\n",
      "INFO 2021-06-13 23:32:25,880 launch.py:266] Local processes completed.\n"
     ]
    }
   ],
   "source": [
    "!bash train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# 模型参数保存路径\n",
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step4：提交预测结果\n",
    "\n",
    "加载训练保存的模型加载后进行预测。\n",
    "\n",
    "**NOTE:** 注意设置用于预测的模型参数路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====start training of 0 epochs=====\n",
      "epoch: 0 / 10, steps: 0 / 312, loss: 0.747075, speed: 31.46 step/s\n",
      "epoch: 0 / 10, steps: 50 / 312, loss: 0.414792, speed: 1.13 step/s\n",
      "epoch: 0 / 10, steps: 100 / 312, loss: 0.141433, speed: 1.13 step/s\n",
      "epoch: 0 / 10, steps: 150 / 312, loss: 0.095784, speed: 1.13 step/s\n",
      "epoch: 0 / 10, steps: 200 / 312, loss: 0.063847, speed: 1.13 step/s\n",
      "epoch: 0 / 10, steps: 250 / 312, loss: 0.046996, speed: 1.13 step/s\n",
      "epoch: 0 / 10, steps: 300 / 312, loss: 0.036301, speed: 1.13 step/s\n",
      "epoch time footprint: 0 hour 4 min 36 sec\n",
      "\n",
      "=====start training of 1 epochs=====\n",
      "epoch: 1 / 10, steps: 38 / 312, loss: 0.029120, speed: 1.14 step/s\n",
      "epoch: 1 / 10, steps: 88 / 312, loss: 0.023712, speed: 1.13 step/s\n",
      "epoch: 1 / 10, steps: 138 / 312, loss: 0.020629, speed: 1.13 step/s\n",
      "epoch: 1 / 10, steps: 188 / 312, loss: 0.019298, speed: 1.13 step/s\n",
      "epoch: 1 / 10, steps: 238 / 312, loss: 0.014880, speed: 1.13 step/s\n",
      "epoch: 1 / 10, steps: 288 / 312, loss: 0.014145, speed: 1.13 step/s\n",
      "epoch time footprint: 0 hour 4 min 35 sec\n",
      "\n",
      "=====start training of 2 epochs=====\n",
      "epoch: 2 / 10, steps: 26 / 312, loss: 0.013381, speed: 1.14 step/s\n",
      "epoch: 2 / 10, steps: 76 / 312, loss: 0.012846, speed: 1.14 step/s\n",
      "epoch: 2 / 10, steps: 126 / 312, loss: 0.011116, speed: 1.14 step/s\n",
      "epoch: 2 / 10, steps: 176 / 312, loss: 0.010983, speed: 1.13 step/s\n",
      "epoch: 2 / 10, steps: 226 / 312, loss: 0.010210, speed: 1.13 step/s\n",
      "epoch: 2 / 10, steps: 276 / 312, loss: 0.010038, speed: 1.14 step/s\n",
      "epoch time footprint: 0 hour 4 min 34 sec\n",
      "\n",
      "=====start training of 3 epochs=====\n",
      "epoch: 3 / 10, steps: 14 / 312, loss: 0.009403, speed: 1.13 step/s\n",
      "epoch: 3 / 10, steps: 64 / 312, loss: 0.008410, speed: 1.14 step/s\n",
      "epoch: 3 / 10, steps: 114 / 312, loss: 0.008090, speed: 1.13 step/s\n",
      "epoch: 3 / 10, steps: 164 / 312, loss: 0.008218, speed: 1.09 step/s\n",
      "epoch: 3 / 10, steps: 214 / 312, loss: 0.008047, speed: 1.11 step/s\n",
      "epoch: 3 / 10, steps: 264 / 312, loss: 0.007466, speed: 1.13 step/s\n",
      "epoch time footprint: 0 hour 4 min 37 sec\n",
      "\n",
      "=====start training of 4 epochs=====\n",
      "epoch: 4 / 10, steps: 2 / 312, loss: 0.006285, speed: 1.14 step/s\n",
      "epoch: 4 / 10, steps: 52 / 312, loss: 0.006990, speed: 1.13 step/s\n",
      "epoch: 4 / 10, steps: 102 / 312, loss: 0.006596, speed: 1.13 step/s\n",
      "epoch: 4 / 10, steps: 152 / 312, loss: 0.006931, speed: 1.13 step/s\n",
      "epoch: 4 / 10, steps: 202 / 312, loss: 0.006288, speed: 1.14 step/s\n",
      "epoch: 4 / 10, steps: 252 / 312, loss: 0.005674, speed: 1.13 step/s\n",
      "epoch: 4 / 10, steps: 302 / 312, loss: 0.005991, speed: 1.14 step/s\n",
      "epoch time footprint: 0 hour 4 min 35 sec\n",
      "\n",
      "=====start training of 5 epochs=====\n",
      "epoch: 5 / 10, steps: 40 / 312, loss: 0.005803, speed: 1.13 step/s\n",
      "epoch: 5 / 10, steps: 90 / 312, loss: 0.005256, speed: 1.13 step/s\n",
      "epoch: 5 / 10, steps: 140 / 312, loss: 0.005509, speed: 1.14 step/s\n",
      "epoch: 5 / 10, steps: 190 / 312, loss: 0.004650, speed: 1.14 step/s\n",
      "epoch: 5 / 10, steps: 240 / 312, loss: 0.005058, speed: 1.14 step/s\n",
      "epoch: 5 / 10, steps: 290 / 312, loss: 0.004977, speed: 1.14 step/s\n",
      "epoch time footprint: 0 hour 4 min 34 sec\n",
      "\n",
      "=====start training of 6 epochs=====\n",
      "epoch: 6 / 10, steps: 28 / 312, loss: 0.004683, speed: 1.14 step/s\n",
      "epoch: 6 / 10, steps: 78 / 312, loss: 0.003887, speed: 1.13 step/s\n",
      "epoch: 6 / 10, steps: 128 / 312, loss: 0.004845, speed: 1.13 step/s\n",
      "epoch: 6 / 10, steps: 178 / 312, loss: 0.004817, speed: 1.13 step/s\n",
      "epoch: 6 / 10, steps: 228 / 312, loss: 0.004375, speed: 1.14 step/s\n",
      "epoch: 6 / 10, steps: 278 / 312, loss: 0.004614, speed: 1.13 step/s\n",
      "epoch time footprint: 0 hour 4 min 35 sec\n",
      "\n",
      "=====start training of 7 epochs=====\n",
      "epoch: 7 / 10, steps: 16 / 312, loss: 0.003777, speed: 1.13 step/s\n",
      "epoch: 7 / 10, steps: 66 / 312, loss: 0.003981, speed: 1.14 step/s\n",
      "epoch: 7 / 10, steps: 116 / 312, loss: 0.003542, speed: 1.13 step/s\n",
      "epoch: 7 / 10, steps: 166 / 312, loss: 0.003420, speed: 1.13 step/s\n",
      "epoch: 7 / 10, steps: 216 / 312, loss: 0.003810, speed: 1.12 step/s\n",
      "epoch: 7 / 10, steps: 266 / 312, loss: 0.003612, speed: 1.13 step/s\n",
      "epoch time footprint: 0 hour 4 min 35 sec\n",
      "\n",
      "=====start training of 8 epochs=====\n",
      "epoch: 8 / 10, steps: 4 / 312, loss: 0.003804, speed: 1.13 step/s\n",
      "epoch: 8 / 10, steps: 54 / 312, loss: 0.003529, speed: 1.13 step/s\n",
      "epoch: 8 / 10, steps: 104 / 312, loss: 0.003561, speed: 1.13 step/s\n",
      "epoch: 8 / 10, steps: 154 / 312, loss: 0.003297, speed: 1.11 step/s\n",
      "epoch: 8 / 10, steps: 204 / 312, loss: 0.003366, speed: 1.13 step/s\n",
      "epoch: 8 / 10, steps: 254 / 312, loss: 0.003477, speed: 1.14 step/s\n",
      "epoch: 8 / 10, steps: 304 / 312, loss: 0.003539, speed: 1.14 step/s\n",
      "epoch time footprint: 0 hour 4 min 35 sec\n",
      "\n",
      "=====start training of 9 epochs=====\n",
      "epoch: 9 / 10, steps: 42 / 312, loss: 0.002900, speed: 1.12 step/s\n",
      "epoch: 9 / 10, steps: 92 / 312, loss: 0.003208, speed: 1.13 step/s\n",
      "epoch: 9 / 10, steps: 142 / 312, loss: 0.003028, speed: 1.08 step/s\n",
      "epoch: 9 / 10, steps: 192 / 312, loss: 0.004281, speed: 1.13 step/s\n",
      "epoch: 9 / 10, steps: 242 / 312, loss: 0.002780, speed: 1.10 step/s\n",
      "epoch: 9 / 10, steps: 292 / 312, loss: 0.003221, speed: 1.14 step/s\n",
      "epoch time footprint: 0 hour 4 min 39 sec\n",
      "\n",
      "=====start evaluating last ckpt of 3120 steps=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:07<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.004990\n",
      "precision: 58.55\t recall: 40.99\t f1: 48.22\t\n",
      "\n",
      "=====training complete=====\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# Starts training.\n",
    "global_step = 0\n",
    "logging_steps = 50\n",
    "save_steps = 10000\n",
    "num_train_epochs = 10\n",
    "output_dir = 'checkpoints'\n",
    "tic_train = time.time()\n",
    "model.train()\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(\"\\n=====start training of %d epochs=====\" % epoch)\n",
    "    tic_epoch = time.time()\n",
    "    for step, batch in enumerate(train_data_loader):\n",
    "        input_ids, seq_lens, tok_to_orig_start_index, tok_to_orig_end_index, labels = batch\n",
    "        logits = model(input_ids=input_ids)\n",
    "        mask = (input_ids != 0).logical_and((input_ids != 1)).logical_and(\n",
    "            (input_ids != 2))\n",
    "        loss = criterion(logits, labels, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_gradients()\n",
    "        loss_item = loss.numpy().item()\n",
    "\n",
    "        if global_step % logging_steps == 0:\n",
    "            print(\n",
    "                \"epoch: %d / %d, steps: %d / %d, loss: %f, speed: %.2f step/s\"\n",
    "                % (epoch, num_train_epochs, step, steps_by_epoch,\n",
    "                    loss_item, logging_steps / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "\n",
    "        if global_step % save_steps == 0 and global_step != 0:\n",
    "            print(\"\\n=====start evaluating ckpt of %d steps=====\" %\n",
    "                    global_step)\n",
    "            precision, recall, f1 = evaluate(\n",
    "                model, criterion, test_data_loader, eval_file_path, \"predict\")\n",
    "            print(\"precision: %.2f\\t recall: %.2f\\t f1: %.2f\\t\" %\n",
    "                    (100 * precision, 100 * recall, 100 * f1))\n",
    "            print(\"saving checkpoing model_%d.pdparams to %s \" %\n",
    "                    (global_step, output_dir))\n",
    "            paddle.save(model.state_dict(),\n",
    "                        os.path.join(output_dir, \n",
    "                                        \"model_%d.pdparams\" % global_step))\n",
    "            model.train()\n",
    "\n",
    "        global_step += 1\n",
    "    tic_epoch = time.time() - tic_epoch\n",
    "    print(\"epoch time footprint: %d hour %d min %d sec\" %\n",
    "            (tic_epoch // 3600, (tic_epoch % 3600) // 60, tic_epoch % 60))\n",
    "\n",
    "# Does final evaluation.\n",
    "print(\"\\n=====start evaluating last ckpt of %d steps=====\" %\n",
    "        global_step)\n",
    "precision, recall, f1 = evaluate(model, criterion, test_data_loader,\n",
    "                                    eval_file_path, \"eval\")\n",
    "print(\"precision: %.2f\\t recall: %.2f\\t f1: %.2f\\t\" %\n",
    "        (100 * precision, 100 * recall, 100 * f1))\n",
    "paddle.save(model.state_dict(),\n",
    "            os.path.join(output_dir,\n",
    "                            \"model_%d.pdparams\" % global_step))\n",
    "print(\"\\n=====training complete=====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ export CUDA_VISIBLE_DEVICES=0\n",
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ export BATCH_SIZE=8\n",
      "+ BATCH_SIZE=8\n",
      "+ export CKPT=./checkpoints/model_3120.pdparams\n",
      "+ CKPT=./checkpoints/model_3120.pdparams\n",
      "+ export DATASET_FILE=./data/test_data.json\n",
      "+ DATASET_FILE=./data/test_data.json\n",
      "+ python run_duie.py --do_predict --init_checkpoint ./checkpoints/model_3120.pdparams --predict_data_file ./data/test_data.json --max_seq_length 512 --batch_size 8\n",
      "[2021-06-14 00:18:52,770] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/roberta_chn_large.pdparams\n",
      "W0614 00:18:52.772150 15644 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0614 00:18:52.777278 15644 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-14 00:19:00,448] [    INFO] - Found /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/vocab.txt\n",
      "[2021-06-14 00:19:00,470] [    INFO] - Preprocessing data, loaded from ./data/test_data.json\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:04<00:00, 229.46it/s]\n",
      "\n",
      "=====start predicting=====\n",
      "100%|█████████████████████████████████████████| 125/125 [00:34<00:00,  3.67it/s]\n",
      "eval loss: 0.042395\n",
      "=====predicting complete=====\n"
     ]
    }
   ],
   "source": [
    "!bash predict.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "预测结果会被保存在data/predictions.json，data/predictions.json.zip，其格式与原数据集文件一致。\n",
    "\n",
    "之后可以使用官方评估脚本评估训练模型在dev_data.json上的效果。如：\n",
    "\n",
    "```shell\n",
    "python re_official_evaluation.py --golden_file=dev_data.json  --predict_file=predicitons.json.zip [--alias_file alias_dict]\n",
    "```\n",
    "输出指标为Precision, Recall 和 F1，Alias file包含了合法的实体别名，最终评测的时候会使用，这里不予提供。\n",
    "\n",
    "之后在test_data.json上预测，然后预测结果（.zip文件）至[千言评测页面](https://aistudio.baidu.com/aistudio/competition/detail/46)。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Tricks\n",
    "\n",
    "### 尝试更多的预训练模型\n",
    "\n",
    "基线采用的预训练模型为ERNIE，PaddleNLP提供了丰富的预训练模型，如BERT，RoBERTa，Electra，XLNet等\n",
    "参考[预训练模型文档](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers.html)\n",
    "\n",
    "如可以选择RoBERTa large中文模型优化模型效果，只需更换模型和tokenizer即可无缝衔接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from paddlenlp.transformers import RobertaForTokenClassification, RobertaTokenizer\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-wwm-ext-large\",\n",
    "    num_classes=(len(label_map) - 2) * 2 + 2)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-wwm-ext-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型集成\n",
    "\n",
    "使用多个模型进行训练预测，将各个模型预测结果进行融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上基线实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐，及时跟踪最新消息和功能哦**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
