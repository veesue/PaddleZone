{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 情感分析项目总结\n",
    "感谢官方Baseline：[http://aistudio.baidu.com/aistudio/projectdetail/2085599](http://aistudio.baidu.com/aistudio/projectdetail/2085599)\n",
    "\n",
    "感谢江流同学观点抽取任务的代码：[https://aistudio.baidu.com/aistudio/projectdetail/2107758](http://aistudio.baidu.com/aistudio/projectdetail/2107758)\n",
    "\n",
    "感谢Hansen0506同学预测结果融合的思路：[https://aistudio.baidu.com/aistudio/projectdetail/2115867](http://aistudio.baidu.com/aistudio/projectdetail/2115867)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 句子级情感分析\n",
    "由于调参越调越糟，这里只改了Baseline的epochs数。\n",
    "\n",
    "改为epochs=10后，对生成的10个预测结果进行融合，得到最终预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.0 官方Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp\r\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压数据\r\n",
    "!unzip -o datasets/ChnSentiCorp\r\n",
    "!unzip -o datasets/NLPCC14-SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 得到数据集字典\r\n",
    "def open_func(file_path):\r\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\r\n",
    "\r\n",
    "data_dict = {'chnsenticorp': {'test': open_func('ChnSentiCorp/test.tsv'),\r\n",
    "                              'dev': open_func('ChnSentiCorp/dev.tsv'),\r\n",
    "                              'train': open_func('ChnSentiCorp/train.tsv')},\r\n",
    "             'nlpcc14sc': {'test': open_func('NLPCC14-SC/test.tsv'),\r\n",
    "                           'train': open_func('NLPCC14-SC/train.tsv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据集\r\n",
    "from paddle.io import Dataset, DataLoader\r\n",
    "from paddlenlp.data import Pad, Stack, Tuple\r\n",
    "import numpy as np\r\n",
    "label_list = [0, 1]\r\n",
    "\r\n",
    "# 注意，由于token type在此项任务中并没有起作用，因此这里不再考虑，让模型自行填充。\r\n",
    "class MyDataset(Dataset):\r\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\r\n",
    "        super().__init__()\r\n",
    "        self._data = data\r\n",
    "        self._tokenizer = tokenizer\r\n",
    "        self._max_len = max_len\r\n",
    "        self._for_test = for_test\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self._data)\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        samples = self._data[idx].split('\\t')\r\n",
    "        label = samples[-2]\r\n",
    "        text = samples[-1]\r\n",
    "        label = int(label)\r\n",
    "        text = self._tokenizer.encode(text, max_seq_len=self._max_len)['input_ids']\r\n",
    "        if self._for_test:\r\n",
    "            return np.array(text, dtype='int64')\r\n",
    "        else:\r\n",
    "            return np.array(text, dtype='int64'), np.array(label, dtype='int64')\r\n",
    "\r\n",
    "def batchify_fn(for_test=False):\r\n",
    "    if for_test:\r\n",
    "        return lambda samples, fn=Pad(axis=0, pad_val=tokenizer.pad_token_id): np.row_stack([data for data in fn(samples)])\r\n",
    "    else:\r\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "                                        Stack()): [data for data in fn(samples)]\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\r\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\r\n",
    "    shuffle = True if not for_test else False\r\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\r\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "from paddle.static import InputSpec\r\n",
    "\r\n",
    "# 模型和分词\r\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\r\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\r\n",
    "\r\n",
    "# 参数设置\r\n",
    "data_name = 'nlpcc14sc'  # 更改此选项改变数据集\r\n",
    "\r\n",
    "## 训练相关\r\n",
    "epochs = 10\r\n",
    "learning_rate = 2e-5\r\n",
    "batch_size = 8\r\n",
    "max_len = 512\r\n",
    "\r\n",
    "## 数据相关\r\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\r\n",
    "if data_name == 'chnsenticorp':\r\n",
    "    dev_dataloader = get_data_loader(data_dict[data_name]['dev'], tokenizer, batch_size, max_len, for_test=False)\r\n",
    "else:\r\n",
    "    dev_dataloader = None\r\n",
    "\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "label = InputSpec((-1, 2), dtype='int64', name='label')\r\n",
    "model = paddle.Model(model, [input], [label])\r\n",
    "\r\n",
    "# 模型准备\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\r\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[paddle.metric.Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开始训练，save_freq改为1\r\n",
    "model.fit(train_dataloader, dev_dataloader, batch_size, epochs, eval_freq=5, save_freq=1, save_dir='./checkpoints', log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#生成10个预测模型\r\n",
    "for i in range(epochs):\r\n",
    "    # 导入预训练模型\r\n",
    "    checkpoint_path = './checkpoints/{}'.format(i)  # 填写预训练模型的保存路径\r\n",
    "\r\n",
    "    model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\r\n",
    "    input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "    model = paddle.Model(model, input)\r\n",
    "    model.load(checkpoint_path)\r\n",
    "\r\n",
    "    # 导入测试集\r\n",
    "    test_dataloader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_len, for_test=True)\r\n",
    "    # 预测保存\r\n",
    "\r\n",
    "    save_file = {'chnsenticorp': './submission/ChnSentiCorp{}.tsv'.format(i), 'nlpcc14sc': './submission/NLPCC14-SC{}.tsv'.format(i)}\r\n",
    "    predicts = []\r\n",
    "    for batch in test_dataloader:\r\n",
    "        predict = model.predict_batch(batch)\r\n",
    "        predicts += predict[0].argmax(axis=-1).tolist()\r\n",
    "\r\n",
    "    with open(save_file[data_name], 'w', encoding='utf8') as f:\r\n",
    "        f.write(\"index\\tprediction\\n\")\r\n",
    "        for idx, sample in enumerate(data_dict[data_name]['test']):\r\n",
    "            qid = sample.split('\\t')[0]\r\n",
    "            f.write(qid + '\\t' + str(predicts[idx]) + '\\n')\r\n",
    "        f.close()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1 结果融合\n",
    "对生成的10个epochs的预测结果求平均数，取整得到最终预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "path = \"./submission\" #文件夹目录\r\n",
    "files= os.listdir(path)\r\n",
    "d2={} #建立字典存放结果\r\n",
    "\r\n",
    "for file in files:\r\n",
    "    if not os.path.isdir(file):\r\n",
    "        f2 = open(path+\"/\"+file, 'r', encoding='utf-8')\r\n",
    "        next(f2)\r\n",
    "        ls2 = []\r\n",
    "        for line in f2:\r\n",
    "            ls2.append(line.strip().split())\r\n",
    "        for item in ls2:\r\n",
    "            d2[eval(item[0])]=d2.get(eval(item[0]),0)+int(item[1])\r\n",
    "        print(d2)\r\n",
    "\r\n",
    "with open('NLPCC14-SC.tsv', 'w', encoding='utf8') as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for key in d2.keys():\r\n",
    "        f.write(str(key) + '\\t' + str(round(d2[key]/epochs+0.0000001)) + '\\n') #平均数为0.5时，取预测值为1\r\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 目标级情感分析\n",
    "这里将官方Baseline的预训练模型改为RoBERTa模型，并对SE-ABSA16_PHNS和SE-ABSA16_CAME的训练集进行了人工清洗与合并。\n",
    "\n",
    "合并后的setrain.tsv已上传至目录内。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.0 使用RoBERTa预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp\r\n",
    "from paddlenlp.transformers import RobertaForSequenceClassification, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压数据\r\n",
    "!unzip -o datasets/SE-ABSA16_CAME\r\n",
    "!unzip -o datasets/SE-ABSA16_PHNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 得到数据集字典\r\n",
    "# 得到数据集字典\r\n",
    "def open_func(file_path):\r\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\r\n",
    "\r\n",
    "data_dict = {'seabsa16phns': {'test': open_func('SE-ABSA16_PHNS/test.tsv'),\r\n",
    "                              'train': open_func('setrain.tsv')},\r\n",
    "             'seabsa16came': {'test': open_func('SE-ABSA16_CAME/test.tsv'),\r\n",
    "                              'train': open_func('setrain.tsv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据集\r\n",
    "from paddle.io import Dataset, DataLoader\r\n",
    "from paddlenlp.data import Pad, Stack, Tuple\r\n",
    "import numpy as np\r\n",
    "label_list = [0, 1]\r\n",
    "\r\n",
    "# 考虑token_type_id\r\n",
    "class MyDataset(Dataset):\r\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\r\n",
    "        super().__init__()\r\n",
    "        self._data = data\r\n",
    "        self._tokenizer = tokenizer\r\n",
    "        self._max_len = max_len\r\n",
    "        self._for_test = for_test\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self._data)\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        samples = self._data[idx].split('\\t')\r\n",
    "        label = samples[-3]\r\n",
    "        text_b = samples[-1]\r\n",
    "        text_a = samples[-2]\r\n",
    "        label = int(label)\r\n",
    "        encoder_out = self._tokenizer.encode(text_a, text_b, max_seq_len=self._max_len)\r\n",
    "        text = encoder_out['input_ids']\r\n",
    "        token_type = encoder_out['token_type_ids']\r\n",
    "        if self._for_test:\r\n",
    "            return np.array(text, dtype='int64'), np.array(token_type, dtype='int64')\r\n",
    "        else:\r\n",
    "            return np.array(text, dtype='int64'), np.array(token_type, dtype='int64'), np.array(label, dtype='int64')\r\n",
    "\r\n",
    "def batchify_fn(for_test=False):\r\n",
    "    if for_test:\r\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "                                        Pad(axis=0, pad_val=tokenizer.pad_token_type_id)): [data for data in fn(samples)]\r\n",
    "    else:\r\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "                                        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\r\n",
    "                                        Stack()): [data for data in fn(samples)]\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\r\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\r\n",
    "    shuffle = True if not for_test else False\r\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\r\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "经调参，learning_rate取1e-5, epochs取10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "from paddle.static import InputSpec\r\n",
    "\r\n",
    "# 模型和分词\r\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-wwm-ext-large', num_classes=2)\r\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-wwm-ext-large')\r\n",
    "\r\n",
    "# 参数设置\r\n",
    "data_name = 'seabsa16came'  # 更改此选项改变数据集\r\n",
    "\r\n",
    "## 训练相关\r\n",
    "epochs = 10\r\n",
    "learning_rate = 1e-5\r\n",
    "batch_size = 8\r\n",
    "max_len = 512\r\n",
    "\r\n",
    "## 数据相关\r\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\r\n",
    "\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "token_type = InputSpec((-1, -1), dtype='int64', name='token_type')\r\n",
    "label = InputSpec((-1, 2), dtype='int64', name='label')\r\n",
    "model = paddle.Model(model, [input, token_type], [label])\r\n",
    "\r\n",
    "print(data_dict[data_name]['train'][-1])\r\n",
    "print(data_dict[data_name]['train'][0])\r\n",
    "\r\n",
    "# 模型准备\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\r\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[paddle.metric.Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开始训练，save_freq改为1\r\n",
    "model.fit(train_dataloader, batch_size=batch_size, epochs=epochs, save_freq=1, save_dir='./checkpoints', log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#生成10个预测模型\r\n",
    "for i in range(epochs):\r\n",
    "    # 导入预训练模型\r\n",
    "    checkpoint_path = './checkpoints/{}'.format(i)  # 填写预训练模型的保存路径\r\n",
    "\r\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-wwm-ext-large', num_classes=2)\r\n",
    "    input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "    token_type = InputSpec((-1, -1), dtype='int64', name='token_type')\r\n",
    "    model = paddle.Model(model, [input, token_type])\r\n",
    "    model.load(checkpoint_path)\r\n",
    "\r\n",
    "    data_name = 'seabsa16phns'  # 切换测试集\r\n",
    "\r\n",
    "    # 导入测试集\r\n",
    "    test_dataloader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_len, for_test=True)\r\n",
    "    # 预测保存\r\n",
    "\r\n",
    "    save_file = {'seabsa16phns': './submission/SE-ABSA16_PHNS{}.tsv'.format(i), 'seabsa16came': './submission/SE-ABSA16_CAME{}.tsv'.format(i)}\r\n",
    "    predicts = []\r\n",
    "    for batch in test_dataloader:\r\n",
    "        predict = model.predict_batch(batch)\r\n",
    "        predicts += predict[0].argmax(axis=-1).tolist()\r\n",
    "\r\n",
    "    with open(save_file[data_name], 'w', encoding='utf8') as f:\r\n",
    "        f.write(\"index\\tprediction\\n\")\r\n",
    "        for idx, sample in enumerate(data_dict[data_name]['test']):\r\n",
    "            qid = sample.split('\\t')[0]\r\n",
    "            f.write(qid + '\\t' + str(predicts[idx]) + '\\n')\r\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**经提交测试，两个数据集都在epochs=5时预测结果最佳，且高于10个epochs合并后的结果**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 观点抽取\n",
    "### 3.0 使用ErnieGram预训练模型\n",
    "这部分代码基本来自江流同学"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp\r\n",
    "from paddlenlp.transformers import ErnieGramTokenizer, ErnieGramForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  datasets/COTE-BD.zip\n",
      "   creating: COTE-BD/\n",
      "  inflating: COTE-BD/train.tsv       \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/COTE-BD/\n",
      "  inflating: __MACOSX/COTE-BD/._train.tsv  \n",
      "  inflating: COTE-BD/License.pdf     \n",
      "  inflating: __MACOSX/COTE-BD/._License.pdf  \n",
      "  inflating: COTE-BD/test.tsv        \n",
      "  inflating: __MACOSX/COTE-BD/._test.tsv  \n",
      "  inflating: __MACOSX/._COTE-BD      \n",
      "Archive:  datasets/COTE-DP.zip\n",
      "   creating: COTE-DP/\n",
      "  inflating: COTE-DP/train.tsv       \n",
      "   creating: __MACOSX/COTE-DP/\n",
      "  inflating: __MACOSX/COTE-DP/._train.tsv  \n",
      "  inflating: COTE-DP/License.pdf     \n",
      "  inflating: __MACOSX/COTE-DP/._License.pdf  \n",
      "  inflating: COTE-DP/test.tsv        \n",
      "  inflating: __MACOSX/COTE-DP/._test.tsv  \n",
      "  inflating: __MACOSX/._COTE-DP      \n",
      "Archive:  datasets/COTE-MFW.zip\n",
      "   creating: COTE-MFW/\n",
      "  inflating: COTE-MFW/train.tsv      \n",
      "   creating: __MACOSX/COTE-MFW/\n",
      "  inflating: __MACOSX/COTE-MFW/._train.tsv  \n",
      "  inflating: COTE-MFW/License.pdf    \n",
      "  inflating: __MACOSX/COTE-MFW/._License.pdf  \n",
      "  inflating: COTE-MFW/test.tsv       \n",
      "  inflating: __MACOSX/COTE-MFW/._test.tsv  \n",
      "  inflating: __MACOSX/._COTE-MFW     \n"
     ]
    }
   ],
   "source": [
    "# 解压数据\r\n",
    "!unzip -o datasets/COTE-BD\r\n",
    "!unzip -o datasets/COTE-DP\r\n",
    "!unzip -o datasets/COTE-MFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 得到数据集字典\r\n",
    "def open_func(file_path):\r\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\r\n",
    "\r\n",
    "data_dict = {'cotebd': {'test': open_func('COTE-BD/test.tsv'),\r\n",
    "                        'train': open_func('COTE-BD/train.tsv')},\r\n",
    "             'cotedp': {'test': open_func('COTE-DP/test.tsv'),\r\n",
    "                        'train': open_func('COTE-DP/train.tsv')},\r\n",
    "             'cotemfw': {'test': open_func('COTE-MFW/test.tsv'),\r\n",
    "                        'train': open_func('COTE-MFW/train.tsv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据集\r\n",
    "from paddle.io import Subset, Dataset, DataLoader\r\n",
    "from paddlenlp.data import Pad, Stack, Tuple\r\n",
    "import numpy as np\r\n",
    "label_list = {'B': 0, 'I': 1, 'O': 2}\r\n",
    "index2label = {0: 'B', 1: 'I', 2: 'O'}\r\n",
    "\r\n",
    "# 考虑token_type_id\r\n",
    "class MyDataset(Dataset):\r\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\r\n",
    "        super().__init__()\r\n",
    "        self._data = data\r\n",
    "        self._tokenizer = tokenizer\r\n",
    "        self._max_len = max_len\r\n",
    "        self._for_test = for_test\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self._data)\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        samples = self._data[idx].split('\\t')\r\n",
    "        label = samples[-2]\r\n",
    "        text = samples[-1]\r\n",
    "        if self._for_test:\r\n",
    "            origin_enc = self._tokenizer.encode(text, max_seq_len=self._max_len)['input_ids']\r\n",
    "            return np.array(origin_enc, dtype='int64')\r\n",
    "        else:\r\n",
    "            \r\n",
    "            # 由于并不是每个字都是一个token，这里采用一种简单的处理方法，先编码label，再编码text中除了label以外的词，最后合到一起\r\n",
    "            texts = text.split(label)\r\n",
    "            label_enc = self._tokenizer.encode(label)['input_ids']\r\n",
    "            cls_enc = label_enc[0]\r\n",
    "            sep_enc = label_enc[-1]\r\n",
    "            label_enc = label_enc[1:-1]\r\n",
    "            \r\n",
    "            # 合并\r\n",
    "            origin_enc = []\r\n",
    "            label_ids = []\r\n",
    "            for index, text in enumerate(texts):\r\n",
    "                text_enc = self._tokenizer.encode(text)['input_ids']\r\n",
    "                text_enc = text_enc[1:-1]\r\n",
    "                origin_enc += text_enc\r\n",
    "                label_ids += [label_list['O']] * len(text_enc)\r\n",
    "                if index != len(texts) - 1:\r\n",
    "                    origin_enc += label_enc\r\n",
    "                    label_ids += [label_list['B']] + [label_list['I']] * (len(label_enc) - 1)\r\n",
    "\r\n",
    "            origin_enc = [cls_enc] + origin_enc + [sep_enc]\r\n",
    "            label_ids = [label_list['O']] + label_ids + [label_list['O']]\r\n",
    "            \r\n",
    "            # 截断\r\n",
    "            if len(origin_enc) > self._max_len:\r\n",
    "                origin_enc = origin_enc[:self._max_len-1] + origin_enc[-1:]\r\n",
    "                label_ids = label_ids[:self._max_len-1] + label_ids[-1:]\r\n",
    "            return np.array(origin_enc, dtype='int64'), np.array(label_ids, dtype='int64')\r\n",
    "\r\n",
    "\r\n",
    "def batchify_fn(for_test=False):\r\n",
    "    if for_test:\r\n",
    "        return lambda samples, fn=Pad(axis=0, pad_val=tokenizer.pad_token_id): np.row_stack([data for data in fn(samples)])\r\n",
    "    else:\r\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "                                        Pad(axis=0, pad_val=label_list['O'])): [data for data in fn(samples)]\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False, k=0):\r\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\r\n",
    "    if for_test == False:\r\n",
    "        shuffle = True\r\n",
    "        train_ds = Subset(dataset=dataset, indices=[i for i in range(len(dataset)) if i % 10 != k])\r\n",
    "        dev_ds = Subset(dataset=dataset, indices=[i for i in range(len(dataset)) if i % 10 == k])\r\n",
    "        train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\r\n",
    "        dev_loader = DataLoader(dataset=dev_ds, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\r\n",
    "        return train_loader, dev_loader\r\n",
    "    else:\r\n",
    "        shuffle = False\r\n",
    "        test_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\r\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ErnieGramForTokenClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-51d9adb67694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 模型和分词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErnieGramForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ernie-gram-zh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErnieGramTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ernie-gram-zh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ErnieGramForTokenClassification' is not defined"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddle.static import InputSpec\r\n",
    "from paddlenlp.metrics import Perplexity\r\n",
    "\r\n",
    "# 模型和分词\r\n",
    "\r\n",
    "model = ErnieGramForTokenClassification.from_pretrained(\"ernie-gram-zh\", num_classes=3)\r\n",
    "tokenizer = ErnieGramTokenizer.from_pretrained('ernie-gram-zh')\r\n",
    "\r\n",
    "# 参数设置\r\n",
    "data_name = 'cotebd'  # 更改此选项改变数据集\r\n",
    "\r\n",
    "## 训练相关\r\n",
    "epochs = 10\r\n",
    "learning_rate = 2e-5\r\n",
    "batch_size = 8\r\n",
    "max_len = 256\r\n",
    "\r\n",
    "## 数据相关\r\n",
    "train_dataloader, dev_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\r\n",
    "\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "label = InputSpec((-1, -1, 3), dtype='int64', name='label')\r\n",
    "model = paddle.Model(model, [input], [label])\r\n",
    "# 模型准备\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\r\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[Perplexity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开始训练，save_freq改为1\r\n",
    "model.fit(train_dataloader, batch_size=batch_size, epochs=epochs, save_freq=1, save_dir='./checkpoints', log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\r\n",
    "\r\n",
    "#生成10个预测结果\r\n",
    "for i in range(epochs):\r\n",
    "    # 导入预训练模型\r\n",
    "    checkpoint_path = './checkpoints/{}'.format()  # 填写预训练模型的保存路径\r\n",
    "\r\n",
    "    model = ErnieGramForTokenClassification.from_pretrained(\"ernie-gram-zh\", num_classes=3)\r\n",
    "\r\n",
    "    input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "    model = paddle.Model(model, [input])\r\n",
    "    model.load(checkpoint_path)\r\n",
    "\r\n",
    "    # 导入测试集\r\n",
    "    test_dataloader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_len, for_test=True)\r\n",
    "    # 预测保存\r\n",
    "\r\n",
    "    predicts = []\r\n",
    "    input_ids = []\r\n",
    "    for batch in test_dataloader:\r\n",
    "        predict = model.predict_batch(batch)\r\n",
    "        predicts += predict[0].argmax(axis=-1).tolist()\r\n",
    "        input_ids += batch.numpy().tolist()\r\n",
    "\r\n",
    "\r\n",
    "    # 先找到B所在的位置，即标号为0的位置，然后顺着该位置一直找到所有的I，即标号为1，即为所得。\r\n",
    "    def find_entity(prediction, input_ids):\r\n",
    "        entity = []\r\n",
    "        entity_ids = []\r\n",
    "        for index, idx in enumerate(prediction):\r\n",
    "            if idx == label_list['B']:\r\n",
    "                entity_ids = [input_ids[index]]\r\n",
    "            elif idx == label_list['I']:\r\n",
    "                if entity_ids:\r\n",
    "                    entity_ids.append(input_ids[index])\r\n",
    "            elif idx == label_list['O']:\r\n",
    "                if entity_ids:\r\n",
    "                    entity_s = ''\r\n",
    "                    for i in entity_ids:\r\n",
    "                        try:\r\n",
    "                            s = tokenizer.convert_ids_to_tokens(i)\r\n",
    "                        except:\r\n",
    "                            s = '[UNK]'\r\n",
    "                        else:\r\n",
    "                            entity_s += s\r\n",
    "                    entity.append(entity_s)\r\n",
    "                    entity_ids = []\r\n",
    "        return entity\r\n",
    "\r\n",
    "\r\n",
    "    save_file = {'cotebd': './submission/COTE_BD{}.tsv'.format(), 'cotedp': './submission/COTE_DP{}.tsv'.format(),\r\n",
    "                 'cotemfw': './submission/COTE_MFW{}.tsv'.format()}\r\n",
    "    with open(save_file[data_name], 'w', encoding='utf8') as f:\r\n",
    "        f.write(\"index\\tprediction\\n\")\r\n",
    "        for idx, sample in enumerate(data_dict[data_name]['test']):\r\n",
    "            qid = sample.split('\\t')[0]\r\n",
    "            entity = find_entity(predicts[idx], input_ids[idx])\r\n",
    "            entity = list(set(entity))  # 去重\r\n",
    "            entity = [re.sub('##', '', e) for e in entity]  # 去除英文编码时的特殊符号\r\n",
    "            entity = [re.sub('[UNK]', '', e) for e in entity]  # 去除未知符号\r\n",
    "            f.write(qid + '\\t' + '\\x01'.join(entity) + '\\n')\r\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 结果融合\n",
    "对生成的10个epochs的预测结果求平均数，取整得到最终预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_name = 'cotebd'\r\n",
    "import os\r\n",
    "path = \"./submission\" #文件夹目录\r\n",
    "files= os.listdir(path)\r\n",
    "\r\n",
    "d2={} #建立字典存放结果\r\n",
    "for i in range(len(data_dict[data_name]['test'])):\r\n",
    "    d2[i]=[]\r\n",
    "\r\n",
    "for file in files:\r\n",
    "    f2 = open(path+\"/\"+file, 'r', encoding='utf-8')\r\n",
    "    next(f2)\r\n",
    "    ls2 = []\r\n",
    "    for line in f2:\r\n",
    "        ls2.append(line.strip().split())\r\n",
    "    for item in ls2:\r\n",
    "        try:\r\n",
    "            d2[eval(item[0])].append(item[1].split('\\x01')) #拆开同一个文件中的多个预测结果，一同存入字典\r\n",
    "        except:\r\n",
    "            pass #排除没有生成结果的情况\r\n",
    "    print(d2)\r\n",
    "\r\n",
    "#整理结果字典格式\r\n",
    "dfinal={} \r\n",
    "for i in range(len(data_dict[data_name]['test'])):\r\n",
    "    dfinal[i]=[]\r\n",
    "for key in d2.keys():\r\n",
    "    for item in d2[key]:\r\n",
    "        for itemitem in item:\r\n",
    "            dfinal[key].append(itemitem)\r\n",
    "print(dfinal)\r\n",
    "\r\n",
    "#取出频率最高的结果\r\n",
    "def maxElement(listA):\r\n",
    "    countA,elementA = 0,''\r\n",
    "    for word in listA:\r\n",
    "        tempCount = listA.count(word) #key\r\n",
    "        if tempCount > countA:\r\n",
    "            countA = tempCount\r\n",
    "            elementA = word\r\n",
    "    return elementA\r\n",
    "\r\n",
    "with open('COTE_BD, 'w', encoding='utf8') as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for i in range(len(data_dict[data_name]['test'])):\r\n",
    "        f.write(str(i) + '\\t' + maxElement(dfinal[i]) + '\\n')\r\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
